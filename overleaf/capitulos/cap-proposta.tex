%%%% CAPÍTULO 3 - PROPOSTA
%%
%% Descreve a proposta do sistema, arquitetura, componentes principais

\chapter{Proposta do Sistema}\label{cap:proposta}

Este capítulo apresenta a proposta do sistema desenvolvido, incluindo sua arquitetura, componentes principais e fluxo de processamento.

\section{Visão geral}\label{sec:visaoGeral}

O sistema proposto é uma solução de análise inteligente de \textit{pipelines} \gls{cicd} do \gls{gitlab} que detecta anomalias e gera recomendações automáticas de otimização, operando exclusivamente com dados de execução coletados via \gls{api}, sem necessidade de acesso ao código-fonte ou arquivos de configuração.

\section{Arquitetura proposta}\label{sec:arquiteturaProposta}

A arquitetura do sistema segue um \textit{pipeline} de processamento modular, conforme ilustrado na \autoref{fig:arquitetura}.

\begin{figure}[htpb]
\centering
\caption{Arquitetura do sistema}
\label{fig:arquitetura}
\begin{center}
\begin{tabular}{@{}c@{\hspace{0.4cm}$\rightarrow$\hspace{0.4cm}}c@{\hspace{0.4cm}$\rightarrow$\hspace{0.4cm}}c@{\hspace{0.4cm}$\rightarrow$\hspace{0.4cm}}c@{\hspace{0.4cm}$\rightarrow$\hspace{0.4cm}}c@{}}
\textbf{Collector} & 
\textbf{Processor} & 
\textbf{Detector} & 
\textbf{Recommender} & 
\textbf{Dashboard} \\
\hline
\small\texttt{\gls{gitlab} \gls{api}} & 
\small\texttt{\gls{postgresql}} & 
\small\texttt{Scores} & 
\small\texttt{Regras + \gls{ml}} & 
\small\texttt{\gls{html} único} \\
\end{tabular}
\end{center}
\fonte{Autora}
\end{figure}

\subsection{Collector}\label{subsec:collector}

O módulo Collector é responsável pela coleta de dados do \gls{gitlab} via \gls{api} \gls{rest} \cite{gitlabapi}, implementando a etapa de Extração (\textit{Extract}), Extração e Carga \gls{etl} \cite{kimball2004data}. A implementação utiliza requisições \gls{http} GET com autenticação via token (\textit{header} \texttt{PRIVATE-TOKEN}) e implementa paginação automática (parâmetros \texttt{page} e \mbox{\texttt{per\_page}}).

\textbf{Implementação técnica:} O coletor utiliza a biblioteca \texttt{requests} do \gls{python} para realizar requisições \gls{http}. A paginação é implementada através de um \textit{loop} que verifica o \textit{header} \mbox{\texttt{x-next-page}} da resposta para determinar se existem mais páginas. O sistema implementa \textit{rate limiting} com \textit{delay} de 0,1 segundos entre requisições para respeitar os limites da \gls{api} do \gls{gitlab} (600 requisições/minuto para usuários autenticados).

A Listagem 1 apresenta um trecho do código de coleta de \textit{pipelines}, demonstrando a implementação da paginação e \textit{rate limiting}.

\begin{sourcecode}[H]
\caption{Coleta de pipelines com paginação automática}
\label{lst:collector}
\begin{lstlisting}[style=pythonstyle, firstnumber=82]
pipes = []
page = 1
per_page = 100

while True:
    params = {"per_page": per_page, "page": page}
    params.update(params_base)

    r = requests.get(
        f"{Config.GITLAB_API}/projects/{Config.PROJECT_ID}/pipelines",
        params=params,
        headers=headers,
        timeout=60
    )
    r.raise_for_status()

    page_data = r.json()
    if not page_data:
        break

    pipes.extend(page_data)

    # Rate limiting: 0.1s entre requisicoes
    time.sleep(0.1)

    # Verifica se ha mais paginas
    if "x-next-page" not in r.headers:
        break
    page += 1

return pipes
\end{lstlisting}
\fonte{Autora}
\end{sourcecode}

\textbf{Dados coletados:}
\begin{itemize}
    \item \textbf{Metadados de \textit{pipelines}:} \texttt{id} (identificador único), \texttt{status} (\textit{success}, \textit{failed}, \textit{canceled}, \textit{running}), \texttt{ref} (\textit{branch}/\textit{tag}), \texttt{sha} (\textit{commit hash}), \mbox{\texttt{web\_url}} (\gls{url} para visualização), \textit{timestamps} (\mbox{\texttt{created\_at}}, \mbox{\texttt{updated\_at}}, \mbox{\texttt{finished\_at}})
    \item \textbf{Metadados de \textit{jobs}:} \texttt{id}, \mbox{\texttt{pipeline\_id}} (\textit{foreign key}), \texttt{name} (nome do \textit{job}), \texttt{stage} (\textit{build}, \textit{test}, \textit{deploy}, etc.), \texttt{status}, \texttt{duration} (em segundos), \mbox{\texttt{queued\_duration}}, \mbox{\texttt{failure\_reason}} (quando falha), \mbox{\texttt{retry\_count}}, \mbox{\texttt{web\_url}}, \textit{timestamps}
    \item \textbf{Trechos de \textit{logs}:} Coletados via \textit{endpoint} \texttt{/jobs/\{id\}/trace} quando disponíveis, limitados aos últimos 10.000 caracteres para evitar sobrecarga de armazenamento
\end{itemize}

\textbf{Armazenamento:} Os dados são armazenados em formato \textit{append-only} na tabela \mbox{\texttt{pipelines\_raw}} e \mbox{\texttt{jobs\_raw}}. Esta estratégia garante um histórico completo e previne a perda de dados em atualizações. O uso de chaves primárias (\texttt{id}) garante a unicidade dos registros, permitindo que o processo de coleta seja \textit{idempotente} (seguro para reprocessamento). O campo \mbox{\texttt{source\_data}} (tipo \gls{jsonb}) armazena o \textit{payload} completo da \gls{api} para rastreabilidade e análise futura, seguindo o padrão de \textit{data lake} \cite{armbrust2021lakehouse}. O banco de dados utilizado é o \gls{postgresql}.

\subsection{Processor (\gls{etl} Incremental)}\label{subsec:processor}

O módulo Processor implementa \gls{etl} incremental com padrão \textit{Watermark} para processar apenas novos dados \cite{clayton2015watermark,armbrust2020delta}. Características principais:
\begin{itemize}
    \item \textbf{\textit{Watermark} por fonte:} armazena último \textit{timestamp} processado
    \item \textbf{\textit{Append-only}:} dados \textit{raw} nunca são sobrescritos, garantindo histórico completo e rastreabilidade \cite{kleppmann2017designing}
    \item \textbf{\textit{UPSERT} \textit{idempotente}:} agregados e \textit{features} são atualizados \textit{idempotentemente}, permitindo reprocessamento seguro
    \item \textbf{Janela deslizante:} reprocessa apenas últimos N dias para corrigir atrasos e garantir consistência temporal
\end{itemize}

A Listagem 2 apresenta um trecho do código de agregação de métricas diárias, demonstrando o cálculo de percentis, média e taxa de falha.

\begin{sourcecode}[H]
\caption{Agregação de métricas diárias com cálculo de percentis}
\label{lst:etl}
\begin{lstlisting}[style=pythonstyle, firstnumber=395]
# Agrega por (project_id, job_name, day)
metrics = df.groupby(['project_id', 'job_name', 'day']).agg({
    'status': [
        ('builds', lambda x: (x == 'success').sum() + 
                              (x == 'failed').sum()),
        ('fails', lambda x: (x == 'failed').sum())
    ],
    'duration': [
        ('p95_duration', lambda x: x.quantile(0.95) 
         if len(x) > 0 else None),
        ('p99_duration', lambda x: x.quantile(0.99) 
         if len(x) > 0 else None),
        ('avg_duration', 'mean'),
        ('total_duration', 'sum')
    ],
    'retry_count': 'max',
    'failure_reason': lambda x: json.dumps(
        x[x.notna()].value_counts().head(5).to_dict())
}).reset_index()

# Calcula taxa de falha: fail_rate = fails / builds
features['fail_rate'] = features['fails'] / (
    features['builds'] + 1)
\end{lstlisting}
\fonte{Autora}
\end{sourcecode}

O Processor gera agregados diários na tabela \mbox{\texttt{metrics\_daily}}, calculando:
\begin{itemize}
    \item Total de builds e falhas por job/dia
    \item Percentis de duração (p95, p99)
    \item Duração média e total
    \item Máximo de retries
    \item Tipos de erro mais frequentes (agregados em \gls{jsonb})
\end{itemize}

\subsection{Feature Engineering}\label{subsec:featureEngineering}

O sistema constrói \textit{features} agregadas por \mbox{\texttt{entity\_key}} (formato: \mbox{\texttt{project\_id:job\_name}}), seguindo práticas de \textit{Feature Store} \cite{he2017feature,he2017featurestore}. As \textit{features} são armazenadas em:
\begin{itemize}
    \item \mbox{\texttt{features\_offline}}: histórico completo (com \textit{event\_time}) para análise temporal e treinamento de modelos
    \item \mbox{\texttt{features\_online}}: \textit{cache} atual para inferência rápida em tempo real
\end{itemize}

\textit{Features} principais:
\begin{itemize}
    \item \mbox{\texttt{dur\_total}}: percentil 95 da duração (p95\_duration) - representa a duração típica dos \textit{pipelines} mais lentos, ignorando casos extremos
    \item \mbox{\texttt{stage\_build}}, \mbox{\texttt{stage\_test}}, \mbox{\texttt{stage\_deploy}}: estimativas por \textit{stage}
    \item \mbox{\texttt{fail\_rate}}: taxa de falha (\textit{fails} / \textit{builds})
    \item \mbox{\texttt{max\_retries}}: máximo de \textit{retries} observado
\end{itemize}

\subsection{Detector (Stats + \gls{ml})}\label{subsec:detector}

O módulo Detector implementa uma abordagem híbrida combinando análise estatística descritiva (percentis, média, desvio-padrão) e aprendizado de máquina não supervisionado, seguindo o paradigma de \textit{ensemble methods} \cite{breiman2001random} e \textit{unsupervised learning} \cite{bishop2006pattern}.

\subsubsection{Estatística Descritiva}\label{subsubsec:estatisticaRobusta}

O sistema calcula estatísticas descritivas para caracterizar a distribuição dos dados e identificar desvios. As métricas utilizadas incluem:

\begin{itemize}
    \item \textbf{Percentis:} Utilizando o método de interpolação linear (tipo 7 de Hyndman-Fan) \cite{hyndman1996sample}, calculando p50 (mediana), p75, p90, p95, p99. A mediana é preferida à média para distribuições assimétricas, pois é mais robusta a outliers, enquanto os percentis superiores (p95, p99) capturam o comportamento da cauda da distribuição.
    \item \textbf{Média e desvio-padrão:} Calculados utilizando fórmulas amostrais: $\bar{x} = \frac{1}{n}\sum_{i=1}^{n} x_i$ e $s = \sqrt{\frac{1}{n-1}\sum_{i=1}^{n}(x_i - \bar{x})^2}$ (desvio-padrão amostral corrigido). Estas métricas são sensíveis a outliers e assumem distribuição aproximadamente normal para interpretação adequada.
    \item \textbf{Z-score:} Calculado como $z = \frac{x - \mu}{\sigma}$, onde valores $|z| > 2$ indicam desvios significativos (aproximadamente 95\% dos dados em distribuição normal) e $|z| > 3$ indicam desvios extremos (99,7\% dos dados) \cite{pearson1901lines}. O z-score depende da média e desvio-padrão, portanto também é sensível a outliers.
\end{itemize}

\subsubsection{\gls{if}}\label{subsubsec:isolationForestDetector}

O algoritmo \gls{if} \cite{liu2008isolation,liu2012isolation} é implementado utilizando a biblioteca \gls{scikit} \cite{pedregosa2011scikit}. A Listagem 3 apresenta a implementação do treinamento do modelo.

\begin{sourcecode}[H]
\caption{Treinamento do modelo \gls{if}}
\label{lst:isolationforest}
\begin{lstlisting}[style=pythonstyle, firstnumber=105]
# Features selecionadas
feature_cols = ['dur_total', 'stage_build', 
                'stage_test', 'stage_deploy', 
                'fail_rate', 'max_retries']
X = df[feature_cols].fillna(0).values

# Normalização (z-score normalization)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Treina \gls{if}
model = IsolationForest(
    contamination=Config.ML_CONTAMINATION,  # 0.1
    random_state=Config.ML_RANDOM_STATE,    # 42
    n_estimators=200
)

model.fit(X_scaled)

# Predições e scores
predictions = model.predict(X_scaled)
scores = model.score_samples(X_scaled)
\end{lstlisting}
\fonte{Autora}
\end{sourcecode}

Os hiperparâmetros utilizados incluem:
\begin{itemize}
    \item \mbox{\texttt{n\_estimators=200}}: Número de árvores de isolamento. Cada árvore é construída com subamostragem aleatória de $\psi$ observações (padrão: 256).
    \item \texttt{contamination=0.1}: Proporção esperada de anomalias na população. Este parâmetro controla o threshold de decisão: valores menores reduzem falsos positivos mas podem aumentar falsos negativos.
    \item \mbox{\texttt{max\_features}}: Número de features consideradas em cada split (padrão: todas as features).
    \item \mbox{\texttt{random\_state=42}}: Semente para reprodutibilidade.
\end{itemize}

\textbf{Funcionamento algorítmico:} O \gls{if} funciona através do princípio de que anomalias são mais fáceis de isolar do que observações normais. Para cada árvore, o algoritmo seleciona aleatoriamente uma \textit{feature} e um valor de \textit{split}, particionando recursivamente o espaço de \textit{features}. O comprimento do caminho médio (\textit{average path length}) até isolar uma observação é utilizado como \textit{score} de anomalia: $s(x, \psi) = 2^{-\frac{E(h(x))}{c(\psi)}}$, onde $h(x)$ é o comprimento do caminho e $c(\psi)$ é um fator de normalização. Na implementação do \gls{scikit}, o método \mbox{\texttt{score\_samples()}} retorna valores onde quanto menor o valor, mais anômalo é o ponto. Quando o parâmetro \texttt{contamination='auto'}, os \textit{scores} de observações normais (\textit{inliers}) são próximos de 0, enquanto os \textit{scores} de anomalias (\textit{outliers}) são próximos de -1. Valores menores (mais negativos) indicam maior grau de anomalia.

\subsubsection{\gls{kmeans} \textit{Clustering}}\label{subsubsec:kmeansDetector}

O algoritmo \gls{kmeans} \cite{macqueen1967some} é utilizado para agrupamento não supervisionado, implementando a inicialização K-Means++ \cite{arthur2007kmeans} para melhorar a convergência e evitar mínimos locais ruins.

\textbf{Hiperparâmetros:}
\begin{itemize}
    \item \mbox{\texttt{n\_clusters=3}}: Número de \textit{clusters} (\textit{pipelines} curtas, médias, longas). O valor k=3 foi escolhido através de análise de \textit{elbow method} e \textit{silhouette score} \cite{rousseeuw1987silhouettes}.
    \item \texttt{init='k-means++'}: Inicialização inteligente que seleciona centros iniciais distantes entre si, melhorando a convergência.
    \item \mbox{\texttt{max\_iter=300}}: Número máximo de iterações do algoritmo.
    \item \mbox{\texttt{random\_state=42}}: Semente para reprodutibilidade.
\end{itemize}

\textbf{Algoritmo:} \gls{kmeans} minimiza a função objetivo: $J = \sum_{i=1}^{n}\sum_{j=1}^{k} w_{ij}||x_i - \mu_j||^2$, onde $w_{ij} = 1$ se $x_i$ pertence ao \textit{cluster} $j$, e 0 caso contrário. O algoritmo alterna entre: (1) atribuir observações ao \textit{cluster} mais próximo (\textit{assignment step}), (2) recalcular os centróides (\textit{update step}), até convergência ou máximo de iterações. A otimização utiliza a desigualdade triangular \cite{elkan2003using} para acelerar o cálculo de distâncias.

\textbf{Contextualização:} Cada \textit{pipeline} recebe um \textit{label} de \textit{cluster} (0, 1 ou 2), permitindo \textit{thresholds} adaptativos por \textit{cluster}. Por exemplo, uma \textit{pipeline} no \textit{cluster} de "longas" com duração de 30 minutos pode ser normal para seu \textit{cluster}, enquanto uma \textit{pipeline} no \textit{cluster} de "curtas" com a mesma duração seria classificada como anomalia.

O \textit{Model Registry} versiona modelos, transformadores (\textit{scalers}) e \textit{feature schemas}, permitindo reprodutibilidade e evolução incremental \cite{breck2017mlops,sculley2015hidden}. Esta abordagem reduz dívida técnica em sistemas de \gls{ml} e facilita o gerenciamento do ciclo de vida de modelos em produção, seguindo práticas de MLOps \cite{arpteg2018software}.

\subsection{Recommender}\label{subsec:recommender}

O módulo Recommender implementa o padrão Strategy \cite{gamma1995design}, permitindo múltiplas estratégias de recomendação. A estratégia principal implementada é a \textbf{Intelligent Strategy}, que combina múltiplas técnicas de análise.

\subsubsection{Estratégia Intelligent (IA Inteligente)}\label{subsubsec:estrategiaIntelligent}

A estratégia Intelligent Strategy implementa um \textit{pipeline} de aprendizado de máquina não supervisionado seguindo o paradigma de \textit{multi-stage learning} \cite{mitchell1997machine}, combinando técnicas de agrupamento, detecção de anomalias e análise contextual \textit{multi-feature}. O fluxo de processamento segue os seguintes passos:

\begin{enumerate}
    \item \textbf{Preparação de \textit{Features}:} O sistema realiza seleção de \textit{features} baseada em relevância e correlação \cite{hastie2009elements}. \textit{Features} selecionadas incluem: \mbox{\texttt{dur\_total}} (percentil 95 da duração agregada, p95\_duration), \mbox{\texttt{stage\_build}}, \mbox{\texttt{stage\_test}}, \mbox{\texttt{stage\_deploy}} (duração estimada por \textit{stage}), \mbox{\texttt{fail\_rate}} (taxa de falha: $\frac{\textit{fails}}{\textit{builds}}$). As \textit{features} são normalizadas utilizando StandardScaler (z-score normalization: $z = \frac{x - \mu}{\sigma}$) para garantir que todas as variáveis tenham a mesma escala, evitando que \textit{features} com valores maiores dominem o algoritmo de \textit{clustering}.
    
    \item \textbf{\textit{Clustering} (\gls{kmeans}):} Agrupa \textit{pipelines} similares em \textit{clusters} (k=3) utilizando o algoritmo K-Means++ \cite{arthur2007kmeans} para contextualização. O número de \textit{clusters} foi determinado através de análise de \textit{elbow method} e validação via \textit{silhouette coefficient} \cite{rousseeuw1987silhouettes}. A métrica de distância utilizada é a distância euclidiana: $d(x, y) = \sqrt{\sum_{i=1}^{n}(x_i - y_i)^2}$. Os \textit{clusters} identificados representam perfis distintos: (0) \textit{pipelines} curtas (duração < p50), (1) \textit{pipelines} médias (p50 $\leq$ duração < p90), (2) \textit{pipelines} longas (duração $\geq$ p90).

    \item \textbf{Detecção de Anomalias (\gls{if}):} Aplica \gls{if} \cite{liu2008isolation,liu2012isolation} com \textit{contamination} configurável (padrão 0,1) para identificar \textit{outliers}. O método \mbox{\texttt{score\_samples()}} retorna \textit{scores} onde valores menores indicam maior grau de anomalia (observações normais têm \textit{scores} próximos de 0, anomalias têm \textit{scores} próximos de -1). A decisão binária (anomalia/normal) é feita através do método \mbox{\texttt{fit\_predict()}}, que retorna -1 para anomalias e 1 para observações normais.
    
    \item \textbf{Aprendizado de \textit{Thresholds} Adaptativos:} Calcula estatísticas descritivas para cada \textit{feature}, incluindo percentis (p50, p75, p90, p95, p99) utilizando interpolação linear \cite{hyndman1996sample}, média amostral ($\bar{x} = \frac{1}{n}\sum x_i$), e desvio-padrão amostral corrigido ($s = \sqrt{\frac{1}{n-1}\sum(x_i - \bar{x})^2}$). Os percentis são robustos a \textit{outliers}, enquanto a média e desvio-padrão são utilizados para cálculo de z-scores. Os \textit{thresholds} são calculados por \textit{cluster}, permitindo adaptação contextual. Por exemplo, o \textit{threshold} p95 para \mbox{\texttt{dur\_total}} no \textit{cluster} de \textit{pipelines} longas será maior que no \textit{cluster} de \textit{pipelines} curtas, evitando falsos positivos. A Listagem 4 apresenta a implementação do método que aprende \textit{thresholds} adaptativos a partir dos dados.

\begin{sourcecode}[H]
\caption{Aprendizado de thresholds adaptativos}
\label{lst:thresholds}
\begin{lstlisting}[style=pythonstyle, firstnumber=99]
def _learn_thresholds(self, df: pd.DataFrame, 
                      features: List[str]) -> dict:
    """Aprende thresholds dos dados (não hardcoded)"""
    thresholds = {}
    for feat in features:
        if feat in df.columns:
            thresholds[feat] = {
                'p50': df[feat].quantile(0.50),
                'p75': df[feat].quantile(0.75),
                'p90': df[feat].quantile(0.90),
                'p95': df[feat].quantile(0.95),
                'p99': df[feat].quantile(0.99),
                'mean': df[feat].mean(),
                'std': df[feat].std()
            }
    return thresholds
\end{lstlisting}
\fonte{Autora}
\end{sourcecode}
    
    \item \textbf{Análise Contextual \textit{Multi-Feature}:} Para cada anomalia detectada, o sistema realiza análise multivariada \cite{tan2006introduction}, analisando múltiplas \textit{features} simultaneamente. A classificação de severidade segue uma heurística baseada em percentis e z-scores:
    \begin{itemize}
        \item \textbf{NORMAL:} valor $\leq$ p75 ou $|z| < 1.0$
        \item \textbf{MEDIO\_ALTO:} p75 < valor $\leq$ p90 ou $1.0 \leq |z| < 2.0$
        \item \textbf{ALTO:} p90 < valor $\leq$ p95 ou $2.0 \leq |z| < 2.5$
        \item \textbf{MUITO\_ALTO:} p95 < valor $\leq$ p99 ou $2.5 \leq |z| < 3.0$
        \item \textbf{EXTREMO:} valor > p99 ou $|z| \geq 3.0$
    \end{itemize}
    O z-score é calculado por \textit{cluster}: $z = \frac{x - \mu_{cluster}}{\sigma_{cluster}}$, onde $\mu_{cluster}$ e $\sigma_{cluster}$ são a média e desvio-padrão do \textit{cluster} ao qual a \textit{pipeline} pertence. A Listagem 5 apresenta a implementação do cálculo de z-score e classificação de severidade por \textit{feature}.

\begin{sourcecode}[H]
\caption{Cálculo de z-score e análise contextual}
\label{lst:zscore}
\begin{lstlisting}[style=pythonstyle, firstnumber=115]
def _analyze_context(self, pipeline: pd.Series, 
                     thresholds: dict, 
                     features: List[str]) -> dict:
    """Analisa contexto completo (múltiplas features)"""
    context = {}
    
    for feat in features:
        if feat not in pipeline or feat not in thresholds:
            continue
        
        value = pipeline[feat]
        t = thresholds[feat]
        
        # Classifica nível baseado em percentis
        if value > t['p99']:
            level = 'EXTREMO'
        elif value > t['p95']:
            level = 'MUITO_ALTO'
        elif value > t['p90']:
            level = 'ALTO'
        elif value > t['p75']:
            level = 'MEDIO_ALTO'
        else:
            level = 'NORMAL'
        
        # Calcula z-score: z = (x - mu) / sigma
        z_score = 0
        if t['std'] > 0:
            z_score = (value - t['mean']) / t['std']
        
        context[feat] = {
            'value': value,
            'level': level,
            'z_score': z_score,
            'p50': t['p50'],
            'p95': t['p95']
        }
    
    return context
\end{lstlisting}
\fonte{Autora}
\end{sourcecode}
    
    \item \textbf{Identificação de Padrões Contextuais:} O sistema implementa um conjunto de regras baseadas em análise de correlação e padrões observados empiricamente \cite{domingos2012few}. Os padrões são identificados através de condições lógicas sobre múltiplas \textit{features}:
    \begin{itemize}
        \item \textbf{slow\_build\_stable:} Detectado quando $\mbox{\texttt{stage\_build}} \in \{ALTO, MUITO\_ALTO, EXTREMO\}$ E $\mbox{\texttt{fail\_rate}} \in \{NORMAL, MEDIO\_ALTO\}$. Indica dependências que podem ser cacheadas. Ação recomendada: adicionar \texttt{cache} no \gls{gitlab} CI com chave baseada em \mbox{\texttt{CI\_COMMIT\_REF\_SLUG}}.
        \item \textbf{slow\_build\_unstable:} Detectado quando $\mbox{\texttt{stage\_build}} \in \{ALTO, MUITO\_ALTO, EXTREMO\}$ E $\mbox{\texttt{fail\_rate}} \in \{ALTO, MUITO\_ALTO, EXTREMO\}$. Indica recursos insuficientes ou configuração inadequada. Ação recomendada: revisar \textit{timeouts} e recursos alocados.
        \item \textbf{slow\_tests:} Detectado quando $\mbox{\texttt{stage\_test}} \in \{ALTO, MUITO\_ALTO, EXTREMO\}$. Indica execução sequencial de testes. Ação recomendada: adicionar \texttt{parallel:} (2--8) e particionamento usando \textit{sharding} (ex: pytest com \texttt{--shard}).
        \item \textbf{high\_failure:} Detectado quando $\mbox{\texttt{fail\_rate}} \in \{ALTO, MUITO\_ALTO, EXTREMO\}$. Indica \textit{flakiness} ou instabilidade de infraestrutura. Ação recomendada: adicionar \texttt{retry} com condições específicas (\mbox{\texttt{runner\_system\_failure}}, \mbox{\texttt{stuck\_or\_timeout\_failure}}).
    \end{itemize}
    
    \item \textbf{Geração de Recomendações:} Gera recomendações estruturadas baseadas no padrão identificado, seguindo o formato de \textit{actionable insights} \cite{guidotti2018survey}. Cada recomendação inclui:
    \begin{itemize}
        \item \textbf{Categoria:} Classificação hierárquica (Build, Test, Infraestrutura, Confiabilidade, Geral)
        \item \textbf{Ação:} Descrição textual da ação recomendada (ex: "CACHE: Build lento mas estável")
        \item \textbf{Justificativa quantitativa:} Evidências numéricas incluindo z-score ($z = 2.7$), percentis ($p95 = 450s$, valor atual $= 680s$), e comparação com mediana do \textit{cluster}
        \item \textbf{Ganho estimado:} Calculado como diferença entre valor atual e p50 do \textit{cluster}: $\Delta = x_{atual} - p50_{cluster}$. O ganho percentual é: $\Delta\% = \frac{\Delta}{x_{atual}} \times 100$

A Listagem 6 apresenta a implementação do cálculo de ganho estimado e confiança baseada em z-score.

\begin{sourcecode}[H]
\caption{Cálculo de ganho estimado e confiança}
\label{lst:ganho}
\begin{lstlisting}[style=pythonstyle, firstnumber=206]
def _generate_recommendation_from_pattern(
    self, pipeline_id: int, pattern: dict, 
    context: dict, cluster: int) -> Recommendation:
    """Gera recomendação baseada no padrão identificado"""
    
    # Determina feature principal pelo z-score absoluto
    main_feature = max(
        context.items(),
        key=lambda x: abs(x[1].get('z_score', 0))
    )
    feat_name = main_feature[0]
    feat_info = main_feature[1]
    
    # Calcula ganho: Delta = x_atual - p50_cluster
    gain = (feat_info['value'] - feat_info['p50'] 
            if feat_info['value'] > feat_info['p50'] 
            else 0)
    
    # Ganho percentual: Delta% = (Delta / x_atual) * 100
    gain_pct = ((gain / feat_info['value'] * 100) 
                if feat_info['value'] > 0 else 0)
    
    # Confiança baseada em z-score absoluto
    z = abs(feat_info['z_score'])
    confidence = ('ALTA' if z > 2.5 
                  else 'MÉDIA' if z > 2.0 
                  else 'BAIXA')
    
    return Recommendation(
        pipeline_id=pipeline_id,
        estimated_gain_sec=gain,
        estimated_gain_pct=gain_pct,
        confidence=confidence,
        ...
    )
\end{lstlisting}
\fonte{Autora}
\end{sourcecode}
        \item \textbf{Confiança:} Nível calculado baseado no z-score absoluto: BAIXA ($|z| < 2.0$), MÉDIA ($2.0 \leq |z| < 2.5$), ALTA ($|z| \geq 2.5$)
        \item \textbf{Código \gls{yaml}:} \textit{Template} gerado automaticamente seguindo a sintaxe do \gls{gitlab} \gls{cicd} \cite{gitlabapi}, incluindo variáveis de ambiente e configurações específicas
        \item \textbf{Evidências:} \textit{Metadados} contextuais incluindo \textit{cluster} ID, \textit{features} analisadas, \textit{scores} de anomalia, e \textit{timestamps}
    \end{itemize}
\end{enumerate}

\subsubsection{Geração de Código \gls{yaml}}\label{subsubsec:geracaoYAML}

A estratégia gera automaticamente exemplos de código \gls{yaml} do \gls{gitlab} CI para cada tipo de recomendação:

\begin{itemize}
    \item \textbf{\textit{Cache}:} Gera configuração de \textit{cache} com chave baseada em \mbox{\texttt{CI\_COMMIT\_REF\_SLUG}} e \textit{paths} configuráveis
    \item \textbf{Paralelização:} Gera configuração de \texttt{parallel} com número de \textit{workers} e \textit{script} de particionamento (ex: pytest com \textit{sharding})
    \item \textbf{\textit{Retry}:} Gera \textit{template} de \textit{retry} com condições específicas e máximo de tentativas
    \item \textbf{Genérico:} Sugere revisão manual da configuração do \textit{pipeline}
\end{itemize}

Cada recomendação inclui justificativa quantitativa baseada em evidências estatísticas (z-score, percentis) e contexto do \textit{cluster} ao qual a \textit{pipeline} pertence, permitindo \textit{thresholds} adaptativos que reduzem falsos positivos.

\subsection{Dashboard}\label{subsec:dashboard}

O \textit{Dashboard} é gerado como \gls{html} auto-contido com as seguintes seções:
\begin{itemize}
    \item \textbf{Estatísticas gerais:} Métricas agregadas do projeto (total de \textit{pipelines}, taxa de sucesso, duração média)
    \item \textbf{Insights e Análises Detalhadas:} Análises agregadas de \textit{jobs} incluindo \textit{top jobs} que mais falham, motivos de falha mais comuns, tempo médio em fila, e identificação de \textit{jobs} instáveis (\textit{flaky})
    \item \textbf{Problemas Detectados:} \textit{Cards} de anomalias priorizadas por \textit{score}, contendo tipo, problema, detalhes e soluções sugeridas
    \item \textbf{Erros em \textit{Jobs}:} Lista de \textit{jobs} que falharam com trechos de \textit{logs} quando disponíveis
    \item \textbf{Resumo Geral:} Estatísticas consolidadas e contadores de problemas encontrados
    \item \textbf{\textit{Links} diretos:} Para \textit{pipelines}/\textit{jobs} no \gls{gitlab}
\end{itemize}


\section{\gls{api} \gls{rest} e UI Interativa}\label{sec:apiRestUI}

O sistema é exposto através de dois componentes principais:
\begin{itemize}
    \item \textbf{\gls{api} \gls{rest} (\gls{fastapi}):} provê \textit{endpoints} para predições, métricas e inferência. O \gls{fastapi} \cite{fastapi2024} foi escolhido por sua alta \textit{performance} (baseada em \textit{async/await}), validação de tipos e documentação automática (Swagger/OpenAPI).
    \item \textbf{UI Interativa (\gls{streamlit}):} interface web com filtros de data, visualizações e execução de \gls{etl}/treino \cite{streamlit2024}. \gls{streamlit} permite desenvolvimento rápido de interfaces de dados interativas sem necessidade de conhecimento em \textit{frontend}
\end{itemize}

A arquitetura incremental permite processamento sob demanda e atualização contínua sem reprocessar todo o histórico.

\section{Estrutura do banco de dados}\label{sec:estruturaBanco}

O banco de dados \gls{postgresql} \cite{postgresql2024} implementa um esquema relacional otimizado para processamento incremental e versionamento de modelos. A modelagem segue os princípios de \textit{append-only} para dados \textit{raw}, \textit{UPSERT} \textit{idempotente} para agregados e \textit{feature store} para inferência rápida \cite{kleppmann2017designing,stonebraker2005one}.

\subsection{Modelo de Dados}\label{subsec:modeloDados}

A \autoref{fig:modeloBanco} ilustra a estrutura completa do banco de dados:

\begin{figure}[htpb]
\centering
\caption{Modelo de dados do sistema}
\label{fig:modeloBanco}
\begin{center}
\begin{tabular}{ll}
\multicolumn{2}{c}{\textbf{Tabelas Raw (Append-Only)}} \\
\hline
\mbox{\texttt{pipelines\_raw}} & \mbox{\texttt{jobs\_raw}} \\
(id, project\_id, status, ref, sha, & (id, pipeline\_id, project\_id, name, \\
web\_url, created\_at, finished\_at, & stage, status, duration, \\
source\_data, ingested\_at) & failure\_reason, retry\_count, \\
 & web\_url, created\_at, finished\_at, \\
 & source\_data, ingested\_at) \\
\hline
\multicolumn{2}{c}{\textbf{Controle de Processamento}} \\
\hline
\mbox{\texttt{processing\_watermarks}} & \mbox{\texttt{kv\_config}} \\
(source, last\_ts, updated\_at) & (key, value, updated\_at) \\
\hline
\multicolumn{2}{c}{\textbf{Agregados e Features}} \\
\hline
\mbox{\texttt{metrics\_daily}} & \mbox{\texttt{features\_offline}} \\
(project\_id, job\_name, day, & (entity\_key, feature\_version, \\
builds, fails, p95\_duration, & payload, event\_time, created\_at) \\
p99\_duration, avg\_duration, & \\
total\_duration, max\_retries, & \mbox{\texttt{features\_online}} \\
error\_types, updated\_at) & (entity\_key, feature\_version, \\
 & payload, updated\_at) \\
\hline
\multicolumn{2}{c}{\textbf{Model Registry e Predições}} \\
\hline
\mbox{\texttt{model\_registry}} & \texttt{predictions} \\
(model\_version, feature\_version, & (run\_id, model\_version, \\
model\_type, model\_path, & feature\_version, prediction, \\
transformer\_path, feature\_schema\_path, & score, label, metadata, \\
training\_window\_start, training\_window\_end, & created\_at) \\
metrics, is\_current, created\_at, & \\
trained\_by) & \mbox{\texttt{predictions\_backfill}} \\
 & (run\_id, model\_version, \\
 & feature\_version, prediction, \\
 & score, label, metadata, \\
 & created\_at) \\
\end{tabular}
\end{center}
\fonte{Autora}
\end{figure}

\subsection{Descrição das Tabelas}\label{subsec:descricaoTabelas}

\subsubsection{Tabelas Raw (Append-Only)}\label{subsubsec:tabelasRaw}

\begin{itemize}
    \item \mbox{\texttt{pipelines\_raw}}
    \begin{itemize}
        \item \textbf{Descrição:} Armazena \textit{metadados} brutos de \textit{pipelines} coletados via \gls{api} do \gls{gitlab}.
        \item \textbf{Campos principais:} \texttt{id} (PK), \mbox{\texttt{project\_id}}, \texttt{status}, \texttt{ref}, \texttt{sha}, \mbox{\texttt{web\_url}}, \textit{timestamps} (\mbox{\texttt{created\_at}}, \mbox{\texttt{finished\_at}}), \mbox{\texttt{source\_data}} (\gls{jsonb} com dados completos da \gls{api}), \mbox{\texttt{ingested\_at}} (\textit{timestamp} de ingestão).
        \item \textbf{Índices:} \mbox{\texttt{(project\_id, updated\_at)}} para otimizar consultas incrementais.
    \end{itemize}
    
    \item \mbox{\texttt{jobs\_raw}}
    \begin{itemize}
        \item \textbf{Descrição:} Armazena \textit{metadados} brutos de \textit{jobs} coletados via \gls{api} do \gls{gitlab}.
        \item \textbf{Campos principais:} \texttt{id} (PK), \mbox{\texttt{pipeline\_id}} (FK para \mbox{\texttt{pipelines\_raw}}), \mbox{\texttt{project\_id}}, \texttt{name}, \texttt{stage}, \texttt{status}, \texttt{duration}, \mbox{\texttt{queued\_duration}}, \mbox{\texttt{failure\_reason}}, \mbox{\texttt{retry\_count}}, \mbox{\texttt{web\_url}}, \textit{timestamps}, \mbox{\texttt{source\_data}} (\gls{jsonb}), \mbox{\texttt{ingested\_at}}.
        \item \textbf{Índices:} \mbox{\texttt{(pipeline\_id)}} e \mbox{\texttt{(project\_id, created\_at)}}.
    \end{itemize}
\end{itemize}

\subsubsection{Controle de Processamento}\label{subsubsec:controleProcessamento}

\begin{itemize}
    \item \mbox{\texttt{processing\_watermarks}}
    \begin{itemize}
        \item \textbf{Descrição:} Implementa o padrão Watermark para controle de processamento incremental. Permite processar apenas novos dados desde a última execução.
        \item \textbf{Campos principais:} \texttt{source} (PK, identifica a fonte de dados), \mbox{\texttt{last\_ts}} (último timestamp processado), \mbox{\texttt{updated\_at}} (timestamp da última atualização).
    \end{itemize}
    
    \item \mbox{\texttt{kv\_config}}
    \begin{itemize}
        \item \textbf{Descrição:} Armazena configurações chave-valor do sistema. Usado para armazenar \mbox{\texttt{MODEL\_CURRENT}}, \mbox{\texttt{FEATURE\_VERSION\_CURRENT}} e outras configurações globais.
        \item \textbf{Campos principais:} \texttt{key} (PK), \texttt{value} (valor da configuração), \mbox{\texttt{updated\_at}}.
    \end{itemize}
\end{itemize}

\subsubsection{Agregados e Features}\label{subsubsec:agregadosFeatures}

\begin{itemize}
    \item \mbox{\texttt{metrics\_daily}}
    \begin{itemize}
        \item \textbf{Descrição:} Armazena agregados diários por \textit{job}, calculados pelo \gls{etl} incremental. Atualizado via \textit{UPSERT} \textit{idempotente}.
        \item \textbf{Chave primária composta:} \mbox{\texttt{(project\_id, job\_name, day)}}.
        \item \textbf{Campos principais:} \texttt{builds} (total de \textit{builds}), \texttt{fails} (total de falhas), \mbox{\texttt{p95\_duration}}, \mbox{\texttt{p99\_duration}}, \mbox{\texttt{avg\_duration}}, \mbox{\texttt{total\_duration}}, \mbox{\texttt{max\_retries}}, \mbox{\texttt{error\_types}} (\gls{jsonb} com tipos de erro agregados), \mbox{\texttt{updated\_at}}.
    \end{itemize}
    
    \item \mbox{\texttt{features\_offline}}
    \begin{itemize}
        \item \textbf{Descrição:} Feature Store histórico completo.
        \item \textbf{Chave primária composta:} \mbox{\texttt{(entity\_key, feature\_version)}}.
        \item \textbf{Campos principais:} \mbox{\texttt{entity\_key}} (formato: \mbox{\texttt{project\_id:job\_name}}), \mbox{\texttt{feature\_version}}, \texttt{payload} (\gls{jsonb} com \textit{features} calculadas), \mbox{\texttt{event\_time}} (\textit{timestamp} do evento), \mbox{\texttt{created\_at}}.
        \item \textbf{Índices:} \mbox{\texttt{(entity\_key, event\_time)}} para consultas temporais.
    \end{itemize}
    
    \item \mbox{\texttt{features\_online}}
    \begin{itemize}
        \item \textbf{Descrição:} Feature Store cache atual para inferência rápida. Mantém apenas a versão mais recente de cada \mbox{\texttt{entity\_key}} para inferência em tempo real.
        \item \textbf{Chave primária:} \mbox{\texttt{entity\_key}} (PK).
        \item \textbf{Campos principais:} \mbox{\texttt{entity\_key}}, \mbox{\texttt{feature\_version}}, \texttt{payload} (\gls{jsonb} com \textit{features} atuais), \mbox{\texttt{updated\_at}}.
    \end{itemize}
\end{itemize}

\subsubsection{Model Registry e Predições}\label{subsubsec:modelRegistryPredicoes}

\begin{itemize}
    \item \mbox{\texttt{model\_registry}}
    \begin{itemize}
        \item \textbf{Descrição:} Versionamento de modelos, transformadores e feature schemas. Permite reprodutibilidade e evolução incremental de modelos.
        \item \textbf{Chave primária:} \mbox{\texttt{model\_version}} (PK).
        \item \textbf{Campos principais:} \mbox{\texttt{model\_version}}, \mbox{\texttt{feature\_version}}, \mbox{\texttt{model\_type}}, \mbox{\texttt{model\_path}}, \mbox{\texttt{transformer\_path}}, \mbox{\texttt{feature\_schema\_path}}, \mbox{\texttt{training\_window\_start}}, \mbox{\texttt{training\_window\_end}}, \texttt{metrics} (\gls{jsonb} com métricas de treinamento), \mbox{\texttt{is\_current}} (\textit{flag} indicando modelo atual), \mbox{\texttt{created\_at}}, \mbox{\texttt{trained\_by}}.
    \end{itemize}
    
    \item \texttt{predictions}
    \begin{itemize}
        \item \textbf{Descrição:} Armazena predições imutáveis por execução.
        \item \textbf{Chave primária composta:} \mbox{\texttt{(run\_id, model\_version)}}.
        \item \textbf{Campos principais:} \mbox{\texttt{run\_id}}, \mbox{\texttt{model\_version}}, \mbox{\texttt{feature\_version}}, \texttt{prediction} (\gls{jsonb} com resultado da predição), \texttt{score} (\textit{score} de anomalia), \texttt{label} (classificação), \texttt{metadata} (\gls{jsonb} com \textit{metadados} adicionais), \mbox{\texttt{created\_at}}.
        \item \textbf{Índices:} \mbox{\texttt{(run\_id, created\_at)}} e \mbox{\texttt{(model\_version, created\_at)}}.
    \end{itemize}
    
    \item \mbox{\texttt{predictions\_backfill}}
    \begin{itemize}
        \item \textbf{Descrição:} Armazena predições históricas geradas por backfill. Estrutura idêntica a \texttt{predictions}, permitindo análise retrospectiva sem modificar predições originais.
        \item \textbf{Estrutura:} Idêntica a \texttt{predictions}.
    \end{itemize}
\end{itemize}

\subsection{Relacionamentos e Integridade}\label{subsec:relacionamentos}

O esquema garante integridade referencial através de:
\begin{itemize}
    \item Foreign Key: \mbox{\texttt{jobs\_raw.pipeline\_id}} $\rightarrow$ \mbox{\texttt{pipelines\_raw.id}} com \texttt{ON DELETE CASCADE}
    \item Chaves primárias compostas para garantir unicidade em agregados e features
    \item Índices otimizados para consultas incrementais e temporais
    \item Constraints implícitos via chaves primárias e únicas
\end{itemize}

A estrutura garante idempotência (via UPSERT e chaves primárias), rastreabilidade (via timestamps e versionamento) e suporte eficiente a processamento incremental (via watermarks e índices temporais).

\section{Repositório e Distribuição}\label{sec:repositorio}

O código-fonte do sistema está disponível publicamente para reprodução e extensão:

\begin{itemize}
    \item \textbf{Repositório GitHub:} \url{https://github.com/maiaradnascimento/utfpr-pb-pipeline-insights-gitlab}
    \item \textbf{Docker Hub:} \url{https://hub.docker.com/r/maiaradnascimento/pipeline-optimizer}
    \item \textbf{Overleaf (Documento LaTeX):} Disponível no repositório GitHub acima
\end{itemize}

O repositório inclui:
\begin{itemize}
    \item Código-fonte completo em \gls{python}
    \item Scripts de migração do banco de dados (\mbox{\texttt{sql/migrations/}})
    \item Dockerfile e docker-compose para execução containerizada
    \item Documentação completa no README.md
    \item Modelos treinados e schemas de features versionados
    \item Scripts CLI para coleta, \gls{etl}, treinamento e análise
    \item Documento LaTeX completo do TCC (pasta \texttt{overleaf/})
\end{itemize}

A distribuição via Docker Hub permite execução imediata sem necessidade de build local, facilitando reprodução de experimentos e deploy em ambientes diversos. Os dados gerados e modelos treinados estão disponíveis como imagens Docker no Docker Hub, permitindo reprodução completa dos experimentos.

\section{Projeto de Validação}\label{sec:projetoValidacao}

O sistema foi validado utilizando \textit{pipelines} do projeto \textit{To Be Continuous - Node.js CI/CD Catalog} \cite{tobecontinuous2024}, disponível publicamente em \url{https://gitlab.com/to-be-continuous/node}. Este projeto foi escolhido por apresentar \textit{pipelines} \gls{cicd} reais e representativas, com múltiplos \textit{stages} (\textit{build}, \textit{test}, \textit{deploy}) e \textit{jobs} variados, fornecendo um conjunto de dados robusto para validação do sistema. O modelo foi treinado exclusivamente com dados coletados deste projeto público, garantindo reprodutibilidade e transparência dos resultados apresentados neste trabalho.

\section{Recomendações Corporativas e Modelos Locais}\label{sec:recomendacoesCorporativas}

Uma limitação importante do sistema atual é que as recomendações geradas são baseadas em padrões universais e heurísticas genéricas, não considerando regras e políticas específicas de cada organização. Em ambientes corporativos, diferentes empresas possuem requisitos distintos de compliance, segurança, arquitetura e boas práticas que devem ser refletidas nas recomendações de otimização de pipelines.

\textbf{Necessidade de Modelos Locais:} Para que empresas possam incluir recomendações personalizadas baseadas em suas regras corporativas, é necessário que o sistema seja capaz de aprender e aplicar conhecimento específico da organização. Esta personalização requer modelos treinados ou configurados localmente, utilizando dados e políticas internas da empresa. Ferramentas públicas ou pagas existentes, como Datadog APM, Dynatrace, Harness \gls{cicd} \cite{harness2024}, e ferramentas de MLOps como MLflow \cite{mlflow2024}, geralmente oferecem recomendações genéricas baseadas em melhores práticas da indústria, mas não permitem personalização profunda baseada em regras corporativas específicas sem acesso ao código-fonte ou configurações internas.

\textbf{Abordagem Proposta para Trabalhos Futuros:} Como extensão natural deste trabalho, propõe-se o desenvolvimento de um módulo baseado em \textit{Large Language Models} (LLMs) \cite{vaswani2017attention,brown2020language} para geração de recomendações corporativas personalizadas. Este módulo utilizaria \textit{Retrieval-Augmented Generation} (RAG) \cite{lewis2020retrieval} para acessar uma base de conhecimento corporativa estruturada, incluindo:

\begin{itemize}
    \item \textbf{FAQ corporativo:} Perguntas frequentes sobre políticas de \gls{cicd}, padrões de arquitetura, e regras de \textit{compliance} da organização
    \item \textbf{Base de dados histórica:} Registros de incidentes passados, soluções aplicadas, e decisões arquiteturais documentadas
    \item \textbf{Documentação técnica:} Guias de boas práticas internas, templates de pipelines aprovados, e padrões de código específicos da empresa
    \item \textbf{Regras de negócio:} Políticas de segurança, requisitos de auditoria, e restrições de infraestrutura específicas da organização
\end{itemize}

O LLM seria treinado ou configurado para consultar esta base de conhecimento antes de gerar recomendações, garantindo que as sugestões estejam alinhadas com as políticas e práticas da organização. Esta abordagem permitiria que empresas mantenham controle sobre as recomendações geradas, adaptando o sistema às suas necessidades específicas sem expor código-fonte ou configurações sensíveis, mantendo assim os princípios de privacidade e segurança que fundamentam este trabalho.

