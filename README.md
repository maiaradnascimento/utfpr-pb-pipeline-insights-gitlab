# Pipeline Optimizer - IA para OtimizaÃ§Ã£o de Pipelines DevOps

Sistema de anÃ¡lise inteligente de pipelines CI/CD usando Machine Learning para detectar anomalias e gerar recomendaÃ§Ãµes automÃ¡ticas de otimizaÃ§Ã£o.

## ğŸ¯ PropÃ³sito

Este sistema coleta dados de execuÃ§Ã£o de pipelines GitLab via API, aplica algoritmos de Machine Learning (Isolation Forest, K-Means) para identificar anomalias e gera recomendaÃ§Ãµes automÃ¡ticas para:

- **Reduzir tempo de execuÃ§Ã£o** (cache, paralelizaÃ§Ã£o)
- **Melhorar confiabilidade** (retry, timeout)
- **Otimizar recursos** (configuraÃ§Ã£o de stages e jobs)

## âœ¨ CaracterÃ­sticas

- âœ… **Arquitetura Incremental e Idempotente** - Processa apenas novos dados, nÃ£o reprocessa tudo
- âœ… **Versionamento de Modelos** - Model Registry com versionamento de modelos, transformadores e feature schemas
- âœ… **Feature Store** - Cache offline/online de features para inferÃªncia rÃ¡pida
- âœ… **API REST** - FastAPI com endpoints para prediÃ§Ãµes, mÃ©tricas, erros e inferÃªncia
- âœ… **UI Interativa** - Streamlit com filtros de data e visualizaÃ§Ãµes
- âœ… **AnÃ¡lise de Erros** - Detalhamento de falhas com agregaÃ§Ã£o por tipo e job
- âœ… **Machine Learning** - Isolation Forest + K-Means para detecÃ§Ã£o de anomalias
- âœ… **Watermark Pattern** - Processamento incremental com controle de timestamp
- âœ… **Janelas Deslizantes** - Reprocessa apenas Ãºltimos N dias para corrigir atrasos

## ğŸš€ InÃ­cio RÃ¡pido

### OpÃ§Ã£o 1: Usar Imagem do Docker Hub com Dados (Mais RÃ¡pido!)

```bash
# 1. Baixe a imagem com dados jÃ¡ incluÃ­dos
docker pull maiaradnascimento/pipeline-optimizer:latest-with-data

# 2. Baixe o docker-compose.with-data.yml do repositÃ³rio
# Ou use o exemplo completo abaixo

# 3. Suba tudo (banco + API + UI)
export DOCKER_USER="maiaradnascimento"
export VERSION="latest-with-data"
docker-compose -f docker-compose.with-data.yml up -d

# 4. Aguarde ~15 segundos e acesse:
# API: http://localhost:8000/docs
# UI: http://localhost:8501
```

**Vantagens:**
- âœ… NÃ£o precisa processar dados (jÃ¡ vem tudo pronto)
- âœ… 605+ pipelines jÃ¡ processados
- âœ… Modelos jÃ¡ treinados
- âœ… UI e API funcionando imediatamente

### OpÃ§Ã£o 2: Build Local

```bash
# 1. Configure as credenciais
export PROJECT_ID="seu_project_id"
export TOKEN="seu_token"

# 2. Construa a imagem Docker
make build

# 3. Execute o pipeline
make processar
```

### OpÃ§Ã£o 3: CÃ³digo Fonte

```bash
# 1. Clone o repositÃ³rio
git clone https://github.com/seu-usuario/pipeline-optimizer.git
cd pipeline-optimizer

# 2. Instale dependÃªncias
pip install -r requirements.txt

# 3. Configure credenciais
export PROJECT_ID="seu_project_id"
export TOKEN="seu_token"

# 4. Execute
python src/cli/fetch.py --incremental
python src/cli/etl_incremental.py
```

### PrÃ©-requisitos

- Docker e Docker Compose (para uso com Docker)
- Python 3.11+ (para uso direto do cÃ³digo)
- Token do GitLab com permissÃ£o `read_api`
- Project ID do projeto GitLab a analisar

## ğŸ³ Docker Hub

### Publicar Imagem com Dados do Banco (Recomendado)

Para publicar a imagem **com os dados do banco jÃ¡ incluÃ­dos**:

```bash
# 1. FaÃ§a login no Docker Hub
docker login

# 2. Configure seu usuÃ¡rio
export DOCKER_USER="seu-usuario"
export VERSION="v1.0.0"  # ou "latest"

# 3. Exporte o banco e publique tudo de uma vez
make docker-push-with-data
```

Isso vai:
- Exportar o banco de dados automaticamente
- Construir a imagem com cÃ³digo + modelos + dados + backup do banco
- Publicar no Docker Hub

**ğŸ“š Guia completo**: Veja `PUBLICAR_DOCKER_HUB.md` para instruÃ§Ãµes detalhadas.

### Publicar Imagem no Docker Hub (sem dados)

```bash
# 1. FaÃ§a login no Docker Hub
docker login

# 2. Configure o nome da imagem (substitua 'seu-usuario' pelo seu username)
export DOCKER_USER="seu-usuario"
export IMAGE_NAME="pipeline-optimizer"
export VERSION="latest"  # ou "v1.0.0", etc.

# 3. Build da imagem
docker build -t $DOCKER_USER/$IMAGE_NAME:$VERSION .

# 4. Tag adicional para 'latest' (se nÃ£o for a versÃ£o latest)
docker tag $DOCKER_USER/$IMAGE_NAME:$VERSION $DOCKER_USER/$IMAGE_NAME:latest

# 5. Push para Docker Hub
docker push $DOCKER_USER/$IMAGE_NAME:$VERSION
docker push $DOCKER_USER/$IMAGE_NAME:latest

# Ou use o Makefile (recomendado)
export DOCKER_USER="seu-usuario"
make docker-push  # Faz build e push automaticamente
```

### Baixar e Usar a Imagem do Docker Hub

#### OpÃ§Ã£o A: Imagem com Dados (Recomendado - Tudo Pronto!)

A imagem `-with-data` inclui cÃ³digo, modelos, dados processados e backup completo do banco. **Ideal para uso imediato sem precisar processar dados.**

```bash
# 1. Baixe a imagem com dados
docker pull maiaradnascimento/pipeline-optimizer:latest-with-data

# 2. Baixe o docker-compose.with-data.yml do repositÃ³rio
# Ou crie manualmente (veja exemplo abaixo)

# 3. Configure e suba tudo (banco + API + UI)
export DOCKER_USER="maiaradnascimento"
export VERSION="latest-with-data"
docker-compose -f docker-compose.with-data.yml up -d

# 4. Aguarde ~15 segundos para o banco ser restaurado
sleep 15

# 5. Acesse os serviÃ§os
# API: http://localhost:8000
# API Docs: http://localhost:8000/docs
# UI: http://localhost:8501
```

**O que estÃ¡ incluÃ­do:**
- âœ… CÃ³digo fonte completo
- âœ… Modelos treinados (model_v2.pkl, model_v3.pkl, etc.)
- âœ… Dados processados
- âœ… Backup completo do banco (605+ pipelines)
- âœ… Script de inicializaÃ§Ã£o automÃ¡tica

#### OpÃ§Ã£o B: Imagem sem Dados (Para Processar Seus PrÃ³prios Dados)

```bash
# 1. Baixe a imagem
docker pull maiaradnascimento/pipeline-optimizer:latest

# 2. Crie um docker-compose.yml
cat > docker-compose.yml << 'EOF'
name: pipeline-optimizer

services:
  postgres:
    image: postgres:15-alpine
    environment:
      POSTGRES_DB: pipeline_optimizer
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./sql/migrations:/docker-entrypoint-initdb.d

  pipeline-optimizer-scripts:
    image: maiaradnascimento/pipeline-optimizer:latest
    depends_on:
      - postgres
    environment:
      PROJECT_ID: ${PROJECT_ID}
      TOKEN: ${TOKEN}
      DB_HOST: postgres
      DB_PORT: 5432
      DB_NAME: pipeline_optimizer
      DB_USER: postgres
      DB_PASS: postgres
    volumes:
      - ./dados:/app/dados
      - ./models:/app/models

volumes:
  postgres_data:
EOF

# 3. Configure credenciais
export PROJECT_ID="seu_project_id"
export TOKEN="seu_token"

# 4. Inicie o banco
docker-compose up -d postgres

# 5. Execute comandos
docker-compose run --rm -e PROJECT_ID=$PROJECT_ID -e TOKEN=$TOKEN \
  pipeline-optimizer-scripts python src/cli/fetch.py --incremental
```

#### Exemplo Completo: docker-compose.with-data.yml

```yaml
name: pipeline-optimizer-with-data

services:
  postgres:
    image: postgres:15-alpine
    container_name: pipeline-optimizer-postgres
    environment:
      POSTGRES_DB: pipeline_optimizer
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5

  pipeline-optimizer-scripts:
    image: maiaradnascimento/pipeline-optimizer:latest-with-data
    container_name: pipeline-optimizer-scripts
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      DB_HOST: postgres
      DB_PORT: 5432
      DB_NAME: pipeline_optimizer
      DB_USER: postgres
      DB_PASS: postgres
    command: /app/scripts/init-db.sh && echo "âœ… Banco inicializado!" && tail -f /dev/null

  api:
    image: maiaradnascimento/pipeline-optimizer:latest-with-data
    container_name: pipeline-optimizer-api
    depends_on:
      - postgres
      - pipeline-optimizer-scripts
    environment:
      DB_HOST: postgres
      DB_PORT: 5432
      DB_NAME: pipeline_optimizer
      DB_USER: postgres
      DB_PASS: postgres
    ports:
      - "8000:8000"
    command: /app/scripts/init-db.sh && uvicorn src.api.app:app --host 0.0.0.0 --port 8000

  ui:
    image: maiaradnascimento/pipeline-optimizer:latest-with-data
    container_name: pipeline-optimizer-ui
    depends_on:
      - api
    environment:
      API_BASE_URL: http://api:8000
    ports:
      - "8501:8501"
    command: streamlit run src/ui/app_incremental.py --server.address 0.0.0.0 --server.port 8501

volumes:
  postgres_data:
```

**ğŸ“š Guia completo**: Veja `COMO_USAR_IMAGEM_DOCKER_HUB.md` para instruÃ§Ãµes detalhadas.

### Comandos Makefile para Docker Hub

Os comandos jÃ¡ estÃ£o disponÃ­veis no `Makefile`:

```bash
# Configure seu usuÃ¡rio do Docker Hub
export DOCKER_USER="seu-usuario"

# Build da imagem
make docker-build

# Push para Docker Hub (faz build automaticamente)
make docker-push

# Pull da imagem do Docker Hub
make docker-pull

# Build de versÃ£o especÃ­fica
VERSION=v1.0.0 make docker-build
VERSION=v1.0.0 make docker-push
```

## ğŸ“‹ Comandos DisponÃ­veis

### Processamento

```bash
make processar              # Pipeline completo (dados reais, todos os pipelines)
make processar-rapido      # Pipeline rÃ¡pido (100 pipelines)
make processar-sintetico   # Pipeline com dados sintÃ©ticos (demo)
```

### Setup

```bash
make build                 # ConstrÃ³i imagem Docker
make up                    # Sobe tudo (banco + API + UI incremental)
make down                  # Para tudo
```

### API

```bash
make api-start            # Inicia API FastAPI (http://localhost:8000)
make api-stop             # Para API FastAPI
make api-logs             # Mostra logs da API
```

### UI

```bash
make ui                   # Inicia UI incremental (http://localhost:8501)
# Nota: A UI bÃ¡sica foi substituÃ­da pela UI incremental
```

### Docker Hub

```bash
make docker-build         # Build da imagem Docker
make docker-push          # Push para Docker Hub (faz build automaticamente)
make docker-pull          # Pull da imagem do Docker Hub

# Configure antes:
export DOCKER_USER="seu-usuario"
```

### Limpeza

```bash
make clean                # Remove containers e volumes Docker
```

### DistribuiÃ§Ã£o

```bash
make export-db            # Exporta banco de dados para backup
make import-db FILE=      # Importa banco de dados (ex: FILE=backup.sql.gz)
make dist                 # Cria pacote completo (banco + modelos + cÃ³digo)
```

### Ajuda

```bash
make help                 # Mostra todos os comandos disponÃ­veis
```

## ğŸ—ï¸ Arquitetura

### Componentes Principais

1. **ETL Incremental** (`src/etl/incremental.py`)
   - Watermark por fonte (Ãºltimo timestamp processado)
   - Append-only em raw tables
   - UPSERT idempotente em agregados/features
   - Janela deslizante (reprocessa apenas Ãºltimos N dias)

2. **Model Registry** (`src/ml/registry.py`)
   - Versionamento de modelos (`model_v{N}.pkl`)
   - Versionamento de transformadores (`scaler_v{N}.pkl`)
   - Versionamento de feature schemas (`feature_schema_v{N}.json`)

3. **Feature Store**
   - `features_offline` - HistÃ³rico completo
   - `features_online` - Cache atual para inferÃªncia rÃ¡pida

4. **API REST** (`src/api/app.py`)
   - `GET /predictions` - Busca prediÃ§Ãµes com filtros
   - `POST /infer/{run_id}` - InferÃªncia individual
   - `GET /metrics` - MÃ©tricas diÃ¡rias
   - `GET /model/info` - InformaÃ§Ãµes do modelo atual

5. **UI Streamlit** (`src/ui/app_incremental.py`)
   - Filtros de data (from/to)
   - Modo Atual vs Snapshot
   - VisualizaÃ§Ã£o de prediÃ§Ãµes e mÃ©tricas
   - ExecuÃ§Ã£o de ETL e treino

## ğŸ“Š Fluxo de Dados

```
GitLab API
    â†“
[Coleta Incremental] â†’ pipelines_raw / jobs_raw (append-only)
    â†“
[ETL Incremental] â†’ metrics_daily (UPSERT)
    â†“
[Feature Engineering] â†’ features_offline / features_online
    â†“
[Model Training] â†’ model_v{N}.pkl + transformers
    â†“
[Inference] â†’ predictions (imutÃ¡vel por run_id + model_version)
```

## ğŸ—„ï¸ Banco de Dados

### Setup Inicial

```bash
# Via Docker (recomendado)
make build
make processar  # Cria banco automaticamente

# Ou manualmente
createdb pipeline_optimizer
psql -d pipeline_optimizer -f sql/migrations/001_initial_schema.sql
```

### Estrutura de Tabelas

- **`processing_watermarks`** - Controla Ãºltimo timestamp processado
- **`pipelines_raw` / `jobs_raw`** - Dados brutos (append-only)
- **`metrics_daily`** - Agregados diÃ¡rios (UPSERT idempotente)
- **`features_offline` / `features_online`** - Feature store
- **`predictions`** - PrediÃ§Ãµes (imutÃ¡veis por run_id + model_version)
- **`predictions_backfill`** - Backfill histÃ³rico
- **`model_registry`** - Versionamento de modelos
- **`kv_config`** - ConfiguraÃ§Ãµes (MODEL_CURRENT, etc)

### âš ï¸ Importante: Como Funcionam as PrediÃ§Ãµes

**Por que hÃ¡ apenas 8 prediÃ§Ãµes quando hÃ¡ muitos registros?**

O sistema gera prediÃ§Ãµes por **job** (nÃ£o por pipeline individual):

1. **AgregaÃ§Ã£o por Job**: O sistema agrega todos os pipelines/jobs e cria uma feature por `job_name` Ãºnico
2. **Entity Key**: Cada feature Ã© identificada por `entity_key = "project_id:job_name"`
3. **PrediÃ§Ãµes**: Uma prediÃ§Ã£o Ã© gerada para cada `entity_key` Ãºnico em `features_online`

**Exemplo:**
- 100 pipelines executados
- 8 jobs Ãºnicos (ex: `build`, `test`, `deploy`, `lint`, `security`, `docker`, `k8s`, `notify`)
- **Resultado**: 8 features e 8 prediÃ§Ãµes (uma para cada job)

Isso Ã© o comportamento esperado! As prediÃ§Ãµes representam o comportamento agregado de cada job ao longo do tempo, nÃ£o execuÃ§Ãµes individuais de pipelines.

## ğŸ”„ Processamento Incremental

### Como Funciona

1. **Watermark**: Armazena Ãºltimo timestamp processado por fonte
2. **Append-only**: Dados raw nunca sÃ£o sobrescritos
3. **UPSERT**: Agregados e features sÃ£o atualizados idempotentemente
4. **Janela Deslizante**: Reprocessa apenas Ãºltimos N dias para corrigir atrasos

### Executar ETL Incremental

```bash
# Via Makefile (recomendado)
make processar

# Ou diretamente
python src/cli/etl_incremental.py --reprocess-days 3
```

## ğŸ“ Model Registry

### Treinar Modelo

```bash
# Via Python
python src/ml/train.py \
    --window-start 2025-01-01 \
    --window-end 2025-12-31

# O modelo serÃ¡ salvo como model_v{N+1}.pkl
```

### Backfill

```bash
# Re-scora prediÃ§Ãµes histÃ³ricas
python src/ml/backfill.py \
    --model-version 2 \
    --days 30
```

## ğŸ”Œ API REST

### Iniciar API

```bash
make api-start
# Acesse: http://localhost:8000/docs
```

### Endpoints Principais

- `GET /healthz` - Health check
- `GET /predictions?from=2025-01-01&to=2025-12-31&mode=actual` - Busca prediÃ§Ãµes
- `POST /infer/{run_id}` - Gera prediÃ§Ã£o para um run_id
- `POST /predictions/generate` - Gera prediÃ§Ãµes em lote para todas as features_online
- `GET /metrics?from=2025-01-01&to=2025-12-31` - MÃ©tricas diÃ¡rias
- `GET /errors?from=2025-01-01&to=2025-12-31&limit=100` - Lista erros de pipelines/jobs
- `GET /errors/summary?from=2025-01-01&to=2025-12-31` - Resumo de erros agregados por tipo
- `GET /model/info` - InformaÃ§Ãµes do modelo atual

## ğŸ–¥ï¸ UI Streamlit

### Iniciar UI

```bash
# UI incremental (com filtros de data, requer API)
make ui

# Acesse: http://localhost:8501
```

**Nota**: A UI incremental substitui a UI bÃ¡sica e requer que a API esteja rodando (`make api-start`).

### Funcionalidades

**UI Incremental** (`src/ui/app_incremental.py`):
- Filtros de data (from/to)
- Modo Atual (modelo atual) vs Snapshot (versÃ£o fixa)
- VisualizaÃ§Ã£o de prediÃ§Ãµes e mÃ©tricas
- VisualizaÃ§Ã£o de erros e anÃ¡lise de falhas
- ExecuÃ§Ã£o de ETL incremental
- Treino de modelo
- Backfill
- IntegraÃ§Ã£o com API REST
- GeraÃ§Ã£o de prediÃ§Ãµes em lote

## ğŸ“ Estrutura do Projeto

```
pipeline-optimizer/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ cli/              # Interface de linha de comando
â”‚   â”‚   â”œâ”€â”€ fetch.py      # Coleta dados do GitLab
â”‚   â”‚   â”œâ”€â”€ normalize.py  # Normaliza dados
â”‚   â”‚   â”œâ”€â”€ analyze.py    # AnÃ¡lise estatÃ­stica
â”‚   â”‚   â”œâ”€â”€ recommend.py  # Gera recomendaÃ§Ãµes
â”‚   â”‚   â””â”€â”€ etl_incremental.py  # ETL incremental
â”‚   â”œâ”€â”€ etl/              # ETL incremental
â”‚   â”‚   â””â”€â”€ incremental.py
â”‚   â”œâ”€â”€ ml/               # Machine Learning
â”‚   â”‚   â”œâ”€â”€ registry.py   # Model Registry
â”‚   â”‚   â”œâ”€â”€ train.py      # Treino de modelo
â”‚   â”‚   â””â”€â”€ backfill.py   # Backfill
â”‚   â”œâ”€â”€ api/              # API REST
â”‚   â”‚   â””â”€â”€ app.py        # FastAPI
â”‚   â”œâ”€â”€ ui/               # Interface web
â”‚   â”‚   â””â”€â”€ app_incremental.py  # UI incremental
â”‚   â”œâ”€â”€ strategies/       # Algoritmos de recomendaÃ§Ã£o
â”‚   â”‚   â””â”€â”€ intelligent_strategy.py
â”‚   â””â”€â”€ utils/            # UtilitÃ¡rios
â”‚       â”œâ”€â”€ dashboard.py  # GeraÃ§Ã£o de dashboard
â”‚       â””â”€â”€ synthetic_data.py
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ migrations/       # Migrations do banco
â”‚       â””â”€â”€ 001_initial_schema.sql
â”œâ”€â”€ models/                # Modelos treinados
â”‚   â”œâ”€â”€ model_v{N}.pkl
â”‚   â”œâ”€â”€ transformers/
â”‚   â””â”€â”€ schemas/
â”œâ”€â”€ dados/
â”‚   â”œâ”€â”€ raw/              # Dados brutos (append-only)
â”‚   â””â”€â”€ processed/        # Dados processados
â”œâ”€â”€ docker-compose.yml    # ConfiguraÃ§Ã£o Docker
â”œâ”€â”€ Dockerfile            # Imagem Docker
â”œâ”€â”€ Makefile             # Comandos simplificados
â””â”€â”€ requirements.txt     # DependÃªncias Python
```

## âš™ï¸ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente

```bash
# ObrigatÃ³rias
export PROJECT_ID=26454237
export TOKEN=seu_token_gitlab

# Opcionais
export GITLAB_API=https://gitlab.com/api/v4
export MAX_PIPELINES=100
export DB_NAME=pipeline_optimizer
export DB_USER=postgres
export DB_PASS=postgres
export DB_HOST=localhost
export DB_PORT=5432
export REPROCESS_DAYS=3  # Dias para reprocessar na janela deslizante
```

### ConfiguraÃ§Ã£o do Banco de Dados

O sistema usa PostgreSQL e cria automaticamente o banco e as tabelas na primeira execuÃ§Ã£o via `make db-setup`. As migrations estÃ£o em `sql/migrations/001_initial_schema.sql`.

## ğŸ§ª Testes

```bash
# Testes ETL
pytest tests/test_incremental_etl.py -v

# Testes API
pytest tests/test_infer_api.py -v
```

## ğŸ” AnÃ¡lise de Erros

A API fornece endpoints especÃ­ficos para anÃ¡lise de erros:

### Listar Erros

```bash
# Via API
curl "http://localhost:8000/errors?from=2025-01-01&to=2025-12-31&limit=100"

# Via UI
# Acesse a aba "AnÃ¡lise de Erros" na interface
```

### Resumo de Erros

```bash
# Via API
curl "http://localhost:8000/errors/summary?from=2025-01-01&to=2025-12-31"
```

Os erros incluem:
- Job ID e Pipeline ID
- Nome do job e stage
- Motivo da falha (`failure_reason`)
- Contagem de retries
- URLs para visualizaÃ§Ã£o no GitLab
- Timestamps de criaÃ§Ã£o e finalizaÃ§Ã£o

## ğŸ“š DocumentaÃ§Ã£o Adicional

- `VERIFICACAO_FORMULAS.md` - VerificaÃ§Ã£o de fÃ³rmulas e cÃ¡lculos do sistema
- `COMO_USAR_IMAGEM_DOCKER_HUB.md` - Guia completo para usar a imagem do Docker Hub

## ğŸ¯ Casos de Uso

### 1. AnÃ¡lise Completa (Primeira ExecuÃ§Ã£o)

```bash
export PROJECT_ID=26454237
export TOKEN=seu_token
make processar
```

### 2. AnÃ¡lise RÃ¡pida (Teste)

```bash
make processar-rapido  # Apenas 100 pipelines
```

### 3. Demo com Dados SintÃ©ticos

```bash
make processar-sintetico  # NÃ£o precisa de credenciais GitLab
```

### 4. Processamento Incremental (DiÃ¡rio)

```bash
# Primeira vez: coleta completa
make processar

# PrÃ³ximas vezes: apenas novos dados
make processar  # Detecta automaticamente novos dados via watermark
```

### 5. Gerar PrediÃ§Ãµes em Lote

```bash
# Via API
curl -X POST "http://localhost:8000/predictions/generate"

# Via UI
# Acesse a aba "PrediÃ§Ãµes" e clique em "Gerar PrediÃ§Ãµes"
```

## ğŸ” Resultados

ApÃ³s executar o pipeline, os resultados estarÃ£o em:

- `dados/processed/{PROJECT_ID}/RELATORIO_FINAL.html` - Dashboard HTML completo
- `dados/processed/{PROJECT_ID}/recomendacoes_ia_inteligente.csv` - RecomendaÃ§Ãµes
- `dados/processed/{PROJECT_ID}/figuras/` - GrÃ¡ficos e visualizaÃ§Ãµes

### Acesso via API e UI

Os resultados tambÃ©m podem ser acessados via:
- **API REST**: `http://localhost:8000/docs` - DocumentaÃ§Ã£o interativa Swagger
- **UI Streamlit**: `http://localhost:8501` - Interface web completa

## ğŸ› ï¸ Troubleshooting

### Erro de PermissÃ£o ao Iniciar API

Se vocÃª encontrar o erro:
```
Error response from daemon: error while creating mount source path '/path/to/models': chown /path/to/models: permission denied
```

**SoluÃ§Ã£o:**

```bash
# Crie os diretÃ³rios manualmente com as permissÃµes corretas
mkdir -p models/transformers models/schemas
mkdir -p dados/raw dados/processed
chmod -R 755 models dados

# Ou use o comando do Makefile
make setup-dirs

# Depois tente novamente
make api-start
```

### Porta 8501 jÃ¡ estÃ¡ em uso

Se vocÃª encontrar o erro:
```
Bind for 0.0.0.0:8501 failed: port is already allocated
```

**SoluÃ§Ãµes:**

1. **Usar porta alternativa automaticamente:**
   ```bash
   # O Makefile agora detecta automaticamente e usa porta 8502 se 8501 estiver ocupada
   make up
   ```

2. **Especificar porta manualmente:**
   ```bash
   export STREAMLIT_PORT=8502
   make up
   ```

3. **Parar o processo que estÃ¡ usando a porta:**
   ```bash
   # Ver qual processo estÃ¡ usando a porta
   lsof -i :8501
   
   # Parar o processo (substitua PID pelo nÃºmero do processo)
   kill -9 <PID>
   ```

### Problemas com Docker no macOS

No macOS, o Docker Desktop pode ter problemas com permissÃµes de volumes. Se o problema persistir:

1. Certifique-se de que os diretÃ³rios existem antes de iniciar os serviÃ§os
2. Verifique as configuraÃ§Ãµes de compartilhamento de arquivos no Docker Desktop
3. Tente reiniciar o Docker Desktop

## ğŸ¤ Contribuindo

Este Ã© um projeto de TCC. Para uso interno ou contribuiÃ§Ãµes, consulte a documentaÃ§Ã£o adicional.

## ğŸ“„ LicenÃ§a

Projeto acadÃªmico - TCC.

---

**Pipeline Optimizer** - Otimize seus pipelines CI/CD com IA! ğŸš€

## ğŸ“ Notas de VersÃ£o

### Funcionalidades Principais

- âœ… Processamento incremental com watermark pattern
- âœ… Model Registry com versionamento completo
- âœ… Feature Store (offline/online)
- âœ… API REST completa com documentaÃ§Ã£o Swagger
- âœ… UI Streamlit interativa
- âœ… AnÃ¡lise de erros detalhada
- âœ… GeraÃ§Ã£o de prediÃ§Ãµes em lote
- âœ… Suporte a Docker e Docker Hub