.PHONY: help processar processar-rapido processar-sintetico build up down api-start api-stop api-logs ui ui-incremental clean export-db import-db dist docker-build docker-push docker-pull

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#                       CONFIGURA√á√ÉO DOCKER HUB
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

DOCKER_USER ?= seu-usuario
IMAGE_NAME = pipeline-optimizer
VERSION ?= latest

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#                       MAKEFILE SIMPLIFICADO
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

help: ## üìö Mostra esta ajuda
	@echo ""
	@echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
	@echo "                    COMANDOS DISPON√çVEIS"
	@echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
	@echo ""
	@echo "üîÑ PROCESSAMENTO:"
	@echo ""
	@echo "  make processar          - Pipeline completo (dados reais, todos)"
	@echo "  make processar-rapido   - Pipeline r√°pido (100 pipelines)"
	@echo "  make processar-sintetico - Pipeline com dados sint√©ticos (demo)"
	@echo ""
	@echo "üîß SETUP:"
	@echo ""
	@echo "  make build             - Constr√≥i imagem Docker"
	@echo "  make up                - Sobe tudo (banco + API + UI incremental)"
	@echo "  make down              - Para tudo"
	@echo ""
	@echo "üöÄ API:"
	@echo ""
	@echo "  make api-start         - Inicia API FastAPI (http://localhost:8000)"
	@echo "  make api-stop          - Para API FastAPI"
	@echo "  make api-logs          - Mostra logs da API"
	@echo ""
	@echo "üßπ LIMPEZA:"
	@echo ""
	@echo "  make clean             - Remove containers e volumes Docker"
	@echo ""
	@echo "üì¶ DISTRIBUI√á√ÉO:"
	@echo ""
	@echo "  make export-db         - Exporta banco de dados para backup"
	@echo "  make import-db FILE=   - Importa banco de dados (ex: FILE=backup.sql.gz)"
	@echo "  make dist              - Cria pacote completo (banco + modelos + c√≥digo)"
	@echo ""
	@echo "üê≥ DOCKER HUB:"
	@echo ""
	@echo "  make docker-build          - Build da imagem Docker"
	@echo "  make docker-push          - Push para Docker Hub"
	@echo "  make docker-pull           - Pull da imagem do Docker Hub"
	@echo "  make docker-build-with-data - Build com dados do banco"
	@echo "  make docker-push-with-data  - Push imagem com dados"
	@echo ""
	@echo "   Configure: export DOCKER_USER=seu-usuario"
	@echo ""
	@echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
	@echo ""
	@echo "üí° Vari√°veis de ambiente necess√°rias:"
	@echo ""
	@echo "  export PROJECT_ID=26454237"
	@echo "  export TOKEN=seu_token"
	@echo ""
	@echo "üí° Vari√°veis opcionais:"
	@echo ""
	@echo "  export MAX_PIPELINES=100        # Limite de pipelines"
	@echo "  export DB_NAME=pipeline_optimizer  # Nome do banco"
	@echo "  export DB_USER=postgres          # Usu√°rio do banco"
	@echo "  export DB_PASS=postgres         # Senha do banco"
	@echo ""

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#                     üîÑ PROCESSAMENTO
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

processar: check-env setup-dirs db-setup ## üîÑ Pipeline completo (dados reais, todos os pipelines)
	@echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
	@echo "         üîÑ PIPELINE COMPLETO - DADOS REAIS"
	@echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
	@echo ""
	@$(MAKE) fetch-incremental
	@echo ""
	@$(MAKE) etl-incremental
	@echo ""
	@$(MAKE) normalize
	@echo ""
	@$(MAKE) analyze
	@echo ""
	@$(MAKE) rec
	@echo ""
	@$(MAKE) dashboard
	@echo ""
	@echo "‚úÖ Pipeline completo conclu√≠do!"
	@echo ""
	@echo "üìÑ Resultados em: dados/processed/{PROJECT_ID}/"

processar-rapido: check-env setup-dirs db-setup ## üîÑ Pipeline r√°pido (100 pipelines)
	@echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
	@echo "         ‚ö° PIPELINE R√ÅPIDO - 100 PIPELINES"
	@echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
	@echo ""
	@docker-compose run --rm -e MAX_PIPELINES=100 pipeline-optimizer-scripts python src/cli/fetch.py --incremental
	@echo ""
	@$(MAKE) etl-incremental
	@echo ""
	@$(MAKE) normalize
	@echo ""
	@$(MAKE) analyze
	@echo ""
	@$(MAKE) rec
	@echo ""
	@$(MAKE) dashboard
	@echo ""
	@echo "‚úÖ Pipeline r√°pido conclu√≠do!"

processar-sintetico: setup-dirs db-setup ## üîÑ Pipeline com dados sint√©ticos (demo)
	@echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
	@echo "         üéØ PIPELINE DEMO - DADOS SINT√âTICOS"
	@echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
	@echo ""
	@docker-compose run --rm pipeline-optimizer-scripts python src/utils/synthetic_data.py
	@echo ""
	@$(MAKE) normalize
	@echo ""
	@$(MAKE) analyze
	@echo ""
	@$(MAKE) rec
	@echo ""
	@$(MAKE) dashboard
	@echo ""
	@echo "‚úÖ Pipeline sint√©tico conclu√≠do!"

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#                     üîß SETUP
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

build: ## üîß Constr√≥i imagem Docker
	@echo "üîß Construindo imagem Docker..."
	@docker-compose build
	@echo "‚úÖ Imagem constru√≠da!"

up: build db-setup api-start ## üöÄ Sobe tudo (banco + API + UI incremental)
	@echo ""
	@echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
	@echo "         üöÄ INICIANDO TODOS OS SERVI√áOS"
	@echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
	@echo ""
	@echo "‚úÖ Banco PostgreSQL: rodando"
	@echo "‚úÖ API FastAPI: rodando em http://localhost:8000"
	@echo ""
	@echo "üñ•Ô∏è  Iniciando UI Incremental..."
	@STREAMLIT_PORT=$${STREAMLIT_PORT:-8501}; \
	if lsof -Pi :$$STREAMLIT_PORT -sTCP:LISTEN -t >/dev/null 2>&1; then \
		echo "‚ö†Ô∏è  Porta $$STREAMLIT_PORT j√° est√° em uso. Tentando porta alternativa..."; \
		STREAMLIT_PORT=8502; \
		if lsof -Pi :$$STREAMLIT_PORT -sTCP:LISTEN -t >/dev/null 2>&1; then \
			echo "‚ùå Portas 8501 e 8502 est√£o em uso. Pare o processo ou use: export STREAMLIT_PORT=8503"; \
			exit 1; \
		fi; \
	fi; \
	echo "   Acesse: http://localhost:$$STREAMLIT_PORT"; \
	echo ""; \
	docker-compose run --rm -p $$STREAMLIT_PORT:8501 pipeline-optimizer-scripts streamlit run src/ui/app_incremental.py --server.address 0.0.0.0 --server.port 8501

down: ## üöÄ Para todos os servi√ßos
	@echo "üõë Parando todos os servi√ßos..."
	@docker-compose stop api postgres
	@echo "‚úÖ Todos os servi√ßos parados!"

setup-dirs: ## üîß Cria diret√≥rios necess√°rios
	@mkdir -p dados/raw dados/processed models/transformers models/schemas
	@chmod -R 755 dados models 2>/dev/null || true

check-env: ## üîß Verifica vari√°veis de ambiente (PROJECT_ID, TOKEN)
	@if [ -z "$$PROJECT_ID" ]; then \
		echo "‚ùå PROJECT_ID n√£o definido"; \
		echo "   Execute: export PROJECT_ID=seu_project_id"; \
		exit 1; \
	fi
	@if [ -z "$$TOKEN" ]; then \
		echo "‚ùå TOKEN n√£o definido"; \
		echo "   Execute: export TOKEN=seu_token"; \
		exit 1; \
	fi

db-setup: ## üîß Cria banco e executa migrations
	@echo "üóÑÔ∏è  Configurando banco de dados PostgreSQL..."
	@docker-compose up -d postgres
	@echo "‚è≥ Aguardando PostgreSQL ficar pronto..."
	@sleep 5
	@echo "üìù Executando migrations..."
	@docker-compose exec -T postgres psql -U $$(echo $${DB_USER:-postgres}) -d $$(echo $${DB_NAME:-pipeline_optimizer}) -f /docker-entrypoint-initdb.d/001_initial_schema.sql 2>/dev/null || \
		docker-compose exec -T postgres psql -U $$(echo $${DB_USER:-postgres}) -d $$(echo $${DB_NAME:-pipeline_optimizer}) < /docker-entrypoint-initdb.d/001_initial_schema.sql || \
		echo "‚ö†Ô∏è  Migration pode j√° ter sido executada. Continuando..."
	@echo "‚úÖ Banco configurado!"

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#                     üìä ETAPAS INDIVIDUAIS (internas)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

fetch-incremental: check-env
	@echo "üì• Coletando dados do GitLab (incremental)..."
	@docker-compose run --rm pipeline-optimizer-scripts python src/cli/fetch.py --incremental

etl-incremental: check-env
	@echo "üîÑ Executando ETL Incremental..."
	@docker-compose run --rm pipeline-optimizer-scripts python src/cli/etl_incremental.py --reprocess-days $$(echo $${REPROCESS_DAYS:-3})

normalize:
	@echo "üìä Normalizando dados..."
	@docker-compose run --rm pipeline-optimizer-scripts python src/cli/normalize.py

analyze: normalize
	@echo "üìà Analisando dados..."
	@docker-compose run --rm pipeline-optimizer-scripts python src/cli/analyze.py

rec:
	@echo "ü§ñ Gerando recomenda√ß√µes..."
	@docker-compose run --rm pipeline-optimizer-scripts python src/cli/recommend.py

dashboard:
	@echo "üåê Gerando dashboard..."
	@docker-compose run --rm pipeline-optimizer-scripts python src/utils/dashboard.py

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#                     üöÄ API
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

api-start: db-setup setup-dirs ## üöÄ Inicia API FastAPI (http://localhost:8000)
	@echo "üöÄ Iniciando API FastAPI em http://localhost:8000"
	@docker-compose up -d api
	@echo "‚úÖ API rodando! Acesse: http://localhost:8000/docs"

api-stop: ## üöÄ Para API FastAPI
	@docker-compose stop api
	@echo "‚úÖ API parada"

api-logs: ## üöÄ Mostra logs da API
	@docker-compose logs -f api

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#                     üßπ LIMPEZA
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

clean: ## üßπ Remove containers e volumes Docker
	@echo "üßπ Removendo containers e volumes..."
	@docker-compose down -v
	@echo "‚úÖ Limpeza conclu√≠da!"

export-db: ## üì¶ Exporta banco de dados para distribui√ß√£o
	@echo "üì¶ Exportando banco de dados..."
	@chmod +x scripts/export_database.sh
	@./scripts/export_database.sh database_backup_$$(date +%Y%m%d_%H%M%S).sql.gz
	@echo "‚úÖ Banco exportado!"

import-db: ## üì• Importa banco de dados (use: make import-db FILE=backup.sql.gz)
	@if [ -z "$(FILE)" ]; then \
		echo "‚ùå Especifique o arquivo: make import-db FILE=database_backup.sql.gz"; \
		exit 1; \
	fi
	@echo "üì• Importando banco de dados..."
	@chmod +x scripts/import_database.sh
	@./scripts/import_database.sh $(FILE)
	@echo "‚úÖ Banco importado!"

dist: export-db ## üì¶ Cria pacote completo para distribui√ß√£o (inclui dados e modelos)
	@echo "üì¶ Criando pacote de distribui√ß√£o..."
	@mkdir -p pipeline-optimizer-dist
	@echo "   üìÅ Copiando c√≥digo e configura√ß√µes..."
	@cp -r src sql Dockerfile docker-compose.dist.yml requirements.txt pipeline-optimizer-dist/ 2>/dev/null || true
	@cp docker-compose.dist.yml pipeline-optimizer-dist/docker-compose.yml 2>/dev/null || true
	@echo "   üìÅ Copiando modelos..."
	@cp -r models pipeline-optimizer-dist/ 2>/dev/null || true
	@echo "   üìÅ Copiando dados processados..."
	@cp -r dados pipeline-optimizer-dist/ 2>/dev/null || true
	@LATEST_BACKUP=$$(ls -t database_backup_*.sql.gz 2>/dev/null | head -1); \
	if [ -n "$$LATEST_BACKUP" ]; then \
		echo "   üìÅ Copiando backup do banco..."; \
		cp "$$LATEST_BACKUP" pipeline-optimizer-dist/database_backup.sql.gz; \
	fi
	@cat > pipeline-optimizer-dist/setup.sh << 'EOF' \
	#!/bin/bash\n\
	set -e\n\
	echo "üöÄ Pipeline Optimizer - Setup"\n\
	echo ""\n\
	docker-compose up -d postgres\n\
	echo "‚è≥ Aguardando PostgreSQL..."\n\
	sleep 5\n\
	if [ -f "database_backup.sql.gz" ]; then\n\
		echo "üì• Importando banco de dados..."\n\
		gunzip -c database_backup.sql.gz | docker-compose exec -T postgres psql -U postgres -d pipeline_optimizer\n\
		echo "‚úÖ Banco importado!"\n\
	else\n\
		echo "‚ö†Ô∏è  database_backup.sql.gz n√£o encontrado. Criando banco vazio..."\n\
		docker-compose exec -T postgres psql -U postgres -d pipeline_optimizer -f /docker-entrypoint-initdb.d/001_initial_schema.sql\n\
	fi\n\
	echo "üöÄ Iniciando API..."\n\
	docker-compose up -d api\n\
	echo ""\n\
	echo "‚úÖ Setup conclu√≠do!"\n\
	echo ""\n\
	echo "üìä Servi√ßos dispon√≠veis:"\n\
	echo "   - API: http://localhost:8000"\n\
	echo "   - API Docs: http://localhost:8000/docs"\n\
	echo "   - Health: http://localhost:8000/healthz"\n\
	echo ""\n\
	echo "üñ•Ô∏è  Para iniciar a UI:"\n\
	echo "   docker-compose run --rm -p 8501:8501 pipeline-optimizer-scripts streamlit run src/ui/app_incremental.py --server.address 0.0.0.0 --server.port 8501"\n\
	EOF
	@chmod +x pipeline-optimizer-dist/setup.sh
	@cat > pipeline-optimizer-dist/Makefile << 'EOF' \
	.PHONY: help setup ui fetch etl train api-start api-stop\n\
	\n\
	help: ## üìö Mostra esta ajuda\n\
		@echo ""\n\
		@echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"\n\
		@echo "                    COMANDOS DISPON√çVEIS"\n\
		@echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"\n\
		@echo ""\n\
		@echo "üöÄ SETUP:"\n\
		@echo "  make setup          - Configura banco e inicia servi√ßos"\n\
		@echo "  make ui             - Inicia UI Streamlit (http://localhost:8501)"\n\
		@echo ""\n\
		@echo "üì• DADOS:"\n\
		@echo "  make fetch          - Coleta novos dados do GitLab"\n\
		@echo "  make etl            - Processa dados (ETL incremental)"\n\
		@echo ""\n\
		@echo "‚öôÔ∏è  MODELO:"\n\
		@echo "  make train          - Treina novo modelo com dados dispon√≠veis"\n\
		@echo ""\n\
		@echo "üöÄ API:"\n\
		@echo "  make api-start      - Inicia API (http://localhost:8000)"\n\
		@echo "  make api-stop       - Para API"\n\
		@echo ""\n\
		@echo "üí° Configure antes de usar:"\n\
		@echo "  export PROJECT_ID=seu_project_id"\n\
		@echo "  export TOKEN=seu_token"\n\
		@echo ""\n\
	\n\
	setup: ## üöÄ Setup inicial\n\
		@./setup.sh\n\
	\n\
	ui: ## üñ•Ô∏è  Inicia UI Streamlit\n\
		@docker-compose run --rm -p 8501:8501 pipeline-optimizer-scripts \\\n\
			streamlit run src/ui/app_incremental.py --server.address 0.0.0.0 --server.port 8501\n\
	\n\
	fetch: ## üì• Coleta dados do GitLab\n\
		@if [ -z "$$PROJECT_ID" ] || [ -z "$$TOKEN" ]; then \\\n\
			echo "‚ùå Configure PROJECT_ID e TOKEN primeiro"; \\\n\
			exit 1; \\\n\
		fi\n\
		@echo "üì• Coletando dados do GitLab..."\n\
		@docker-compose run --rm -e PROJECT_ID=$$PROJECT_ID -e TOKEN=$$TOKEN \\\n\
			pipeline-optimizer-scripts python src/cli/fetch.py --incremental\n\
	\n\
	etl: ## üîÑ Processa dados (ETL)\n\
		@if [ -z "$$PROJECT_ID" ]; then \\\n\
			echo "‚ùå Configure PROJECT_ID primeiro"; \\\n\
			exit 1; \\\n\
		fi\n\
		@echo "üîÑ Executando ETL incremental..."\n\
		@docker-compose run --rm -e PROJECT_ID=$$PROJECT_ID \\\n\
			pipeline-optimizer-scripts python src/cli/etl_incremental.py --reprocess-days $$(echo $${REPROCESS_DAYS:-3})\n\
	\n\
	train: ## ‚öôÔ∏è  Treina novo modelo\n\
		@if [ -z "$$PROJECT_ID" ]; then \\\n\
			echo "‚ùå Configure PROJECT_ID primeiro"; \\\n\
			exit 1; \\\n\
		fi\n\
		@echo "‚öôÔ∏è  Treinando modelo..."\n\
		@docker-compose run --rm -e PROJECT_ID=$$PROJECT_ID \\\n\
			pipeline-optimizer-scripts python src/ml/train.py --all\n\
		@echo "‚úÖ Modelo treinado! Reinicie a API: make api-stop && make api-start"\n\
	\n\
	api-start: ## üöÄ Inicia API\n\
		@docker-compose up -d api\n\
		@echo "‚úÖ API rodando em http://localhost:8000"\n\
		@echo "üìö Docs: http://localhost:8000/docs"\n\
	\n\
	api-stop: ## üõë Para API\n\
		@docker-compose stop api\n\
		@echo "‚úÖ API parada"\n\
	EOF
	@cat > pipeline-optimizer-dist/README.md << 'EOF' \
	# Pipeline Optimizer - Distribui√ß√£o Completa\n\
	\n\
	Este pacote inclui:\n\
	- ‚úÖ C√≥digo fonte completo\n\
	- ‚úÖ Modelos pr√©-treinados (v2-v6)\n\
	- ‚úÖ Dados processados\n\
	- ‚úÖ Banco de dados pr√©-populado\n\
	- ‚úÖ Tudo configurado e pronto para uso\n\
	\n\
	## üöÄ In√≠cio R√°pido\n\
	\n\
	### Pr√©-requisitos\n\
	\n\
	- Docker e Docker Compose instalados\n\
	\n\
	### 1. Setup Inicial\n\
	\n\
	\`\`\`bash\n\
	# Execute o script de setup (importa banco e inicia API)\n\
	./setup.sh\n\
	\`\`\`\n\
	\n\
	### 2. Iniciar UI\n\
	\n\
	\`\`\`bash\n\
	make ui\n\
	# Ou manualmente:\n\
	# docker-compose run --rm -p 8501:8501 pipeline-optimizer-scripts \\\n\
	#   streamlit run src/ui/app_incremental.py --server.address 0.0.0.0 --server.port 8501\n\
	\`\`\`\n\
	\n\
	Acesse: **http://localhost:8501**\n\
	\n\
	## üì• Adicionar Novos Dados\n\
	\n\
	### Op√ß√£o 1: Via UI (Recomendado)\n\
	\n\
	1. Acesse a UI: http://localhost:8501\n\
	2. Configure `PROJECT_ID` e `TOKEN` no sidebar\n\
	3. V√° na aba "üì• Obter Dados"\n\
	4. Clique em "Buscar Dados"\n\
	\n\
	### Op√ß√£o 2: Via Makefile\n\
	\n\
	\`\`\`bash\n\
	# Configure credenciais\n\
	export PROJECT_ID="seu_project_id"\n\
	export TOKEN="seu_token"\n\
	\n\
	# Coletar dados\n\
	make fetch\n\
	\n\
	# Processar dados (ETL)\n\
	make etl\n\
	\`\`\`\n\
	\n\
	## ‚öôÔ∏è Treinar Novo Modelo\n\
	\n\
	### Via UI\n\
	\n\
	1. Acesse a UI: http://localhost:8501\n\
	2. V√° na aba "‚öôÔ∏è Treino"\n\
	3. Configure par√¢metros\n\
	4. Clique em "Treinar Modelo"\n\
	\n\
	### Via Makefile\n\
	\n\
	\`\`\`bash\n\
	export PROJECT_ID="seu_project_id"\n\
	make train\n\
	\`\`\`\n\
	\n\
	## üìä Comandos Dispon√≠veis\n\
	\n\
	\`\`\`bash\n\
	make help          # Ver todos os comandos\n\
	make setup         # Setup inicial\n\
	make ui            # Iniciar UI\n\
	make fetch         # Coletar dados do GitLab\n\
	make etl           # Processar dados (ETL)\n\
	make train         # Treinar modelo\n\
	make api-start     # Iniciar API\n\
	make api-stop      # Parar API\n\
	\`\`\`\n\
	\n\
	## üìä O que est√° inclu√≠do\n\
	\n\
	### Modelos Treinados\n\
	- **v6** (atual em produ√ß√£o)\n\
	- v2-v5 (hist√≥rico)\n\
	\n\
	### Dados Processados\n\
	- M√©tricas di√°rias\n\
	- Features offline/online\n\
	- Predi√ß√µes geradas\n\
	- Relat√≥rios e gr√°ficos\n\
	\n\
	### Banco de Dados\n\
	- Dados raw (pipelines e jobs)\n\
	- M√©tricas di√°rias processadas\n\
	- Features offline/online\n\
	- Predi√ß√µes geradas\n\
	- Model registry\n\
	\n\
	## üîß Configura√ß√£o\n\
	\n\
	Para usar com seu pr√≥prio projeto GitLab:\n\
	\n\
	\`\`\`bash\n\
	export PROJECT_ID="seu_project_id"\n\
	export TOKEN="seu_token"\n\
	export GITLAB_API="https://gitlab.com/api/v4"\n\
	\`\`\`\n\
	\n\
	## üÜò Troubleshooting\n\
	\n\
	### Banco n√£o importa\n\
	\`\`\`bash\n\
	docker-compose logs postgres\n\
	gunzip -c database_backup.sql.gz | docker-compose exec -T postgres psql -U postgres -d pipeline_optimizer\n\
	\`\`\`\n\
	\n\
	### API n√£o inicia\n\
	\`\`\`bash\n\
	docker-compose logs api\n\
	docker-compose restart api\n\
	\`\`\`\n\
	\n\
	### Erro ao coletar dados\n\
	- Verifique se `PROJECT_ID` e `TOKEN` est√£o configurados\n\
	- Verifique se o token tem permiss√£o `read_api`\n\
	EOF
	@tar -czf pipeline-optimizer-completo-v1.0.0.tar.gz \
		--exclude='.git' \
		--exclude='*.pyc' \
		--exclude='__pycache__' \
		--exclude='dados/raw' \
		pipeline-optimizer-dist/ 2>/dev/null || true
	@echo ""
	@echo "‚úÖ Pacote criado: pipeline-optimizer-completo-v1.0.0.tar.gz"
	@echo "   üì¶ Inclui: c√≥digo + modelos + dados processados + banco"

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#                     üê≥ DOCKER HUB
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

docker-build: ## üê≥ Build da imagem Docker
	@echo "üê≥ Construindo imagem Docker..."
	@echo "   Usu√°rio: $(DOCKER_USER)"
	@echo "   Imagem: $(IMAGE_NAME)"
	@echo "   Vers√£o: $(VERSION)"
	@docker build -t $(DOCKER_USER)/$(IMAGE_NAME):$(VERSION) .
	@if [ "$(VERSION)" != "latest" ]; then \
		echo "   Criando tag 'latest'..."; \
		docker tag $(DOCKER_USER)/$(IMAGE_NAME):$(VERSION) $(DOCKER_USER)/$(IMAGE_NAME):latest; \
	fi
	@echo "‚úÖ Imagem constru√≠da: $(DOCKER_USER)/$(IMAGE_NAME):$(VERSION)"

docker-push: docker-build ## üê≥ Push para Docker Hub
	@echo "üê≥ Enviando imagem para Docker Hub..."
	@echo "   Usu√°rio: $(DOCKER_USER)"
	@echo "   Imagem: $(IMAGE_NAME)"
	@echo "   Vers√£o: $(VERSION)"
	@docker push $(DOCKER_USER)/$(IMAGE_NAME):$(VERSION)
	@if [ "$(VERSION)" != "latest" ]; then \
		echo "   Enviando tag 'latest'..."; \
		docker push $(DOCKER_USER)/$(IMAGE_NAME):latest; \
	fi
	@echo "‚úÖ Imagem enviada para Docker Hub!"
	@echo "   Pull com: docker pull $(DOCKER_USER)/$(IMAGE_NAME):$(VERSION)"

docker-pull: ## üê≥ Pull da imagem do Docker Hub
	@echo "üê≥ Baixando imagem do Docker Hub..."
	@echo "   Usu√°rio: $(DOCKER_USER)"
	@echo "   Imagem: $(IMAGE_NAME)"
	@echo "   Vers√£o: $(VERSION)"
	@docker pull $(DOCKER_USER)/$(IMAGE_NAME):$(VERSION)
	@echo "‚úÖ Imagem baixada: $(DOCKER_USER)/$(IMAGE_NAME):$(VERSION)"

docker-build-with-data: export-db ## üê≥ Build da imagem Docker com dados do banco
	@echo "üê≥ Construindo imagem Docker com dados do banco..."
	@echo "   Usu√°rio: $(DOCKER_USER)"
	@echo "   Imagem: $(IMAGE_NAME)"
	@echo "   Vers√£o: $(VERSION)"
	@LATEST_BACKUP=$$(ls -t database_backup_*.sql.gz 2>/dev/null | head -1); \
	if [ -z "$$LATEST_BACKUP" ]; then \
		echo "‚ùå Nenhum backup do banco encontrado. Execute 'make export-db' primeiro."; \
		exit 1; \
	fi; \
	echo "   Usando backup: $$LATEST_BACKUP"; \
	cp "$$LATEST_BACKUP" database_backup.sql.gz; \
	echo "   Criando .dockerignore tempor√°rio (incluindo dados e models)..."; \
	cp .dockerignore .dockerignore.bak 2>/dev/null || touch .dockerignore.bak; \
	sed '/^dados\//d; /^models\//d' .dockerignore.bak > .dockerignore.tmp 2>/dev/null || echo "" > .dockerignore.tmp; \
	mv .dockerignore.tmp .dockerignore; \
	docker build -f Dockerfile.with-data -t $(DOCKER_USER)/$(IMAGE_NAME):$(VERSION)-with-data .; \
	BUILD_EXIT=$$?; \
	rm -f database_backup.sql.gz; \
	mv .dockerignore.bak .dockerignore 2>/dev/null || rm -f .dockerignore; \
	if [ $$BUILD_EXIT -ne 0 ]; then exit $$BUILD_EXIT; fi; \
	if [ "$(VERSION)" != "latest" ]; then \
		echo "   Criando tag 'latest-with-data'..."; \
		docker tag $(DOCKER_USER)/$(IMAGE_NAME):$(VERSION)-with-data $(DOCKER_USER)/$(IMAGE_NAME):latest-with-data; \
	fi; \
	echo "‚úÖ Imagem constru√≠da: $(DOCKER_USER)/$(IMAGE_NAME):$(VERSION)-with-data"

docker-push-with-data: docker-build-with-data ## üê≥ Push da imagem com dados para Docker Hub
	@echo "üê≥ Enviando imagem com dados para Docker Hub..."
	@echo "   Usu√°rio: $(DOCKER_USER)"
	@echo "   Imagem: $(IMAGE_NAME)"
	@echo "   Vers√£o: $(VERSION)-with-data"
	@docker push $(DOCKER_USER)/$(IMAGE_NAME):$(VERSION)-with-data
	@if [ "$(VERSION)" != "latest" ]; then \
		echo "   Enviando tag 'latest-with-data'..."; \
		docker push $(DOCKER_USER)/$(IMAGE_NAME):latest-with-data; \
	fi
	@echo "‚úÖ Imagem com dados enviada para Docker Hub!"
	@echo "   Pull com: docker pull $(DOCKER_USER)/$(IMAGE_NAME):$(VERSION)-with-data"
